[{"title":"MongoDB 分片","date":"2016-12-12T08:00:22.000Z","path":"2016/12/12/MongoDB-分片/","text":"MongoDB 分片1. 加host1234vi /etc/hosts192.168.130.93 mongo1192.168.130.94 mongo2192.168.130.95 mongo3 2. 创建目录123456mongo1mkdir -p /data/shard1_1 /data/shard2_1 /data/shard3_1 /data/configmongo2mkdir -p /data/shard1_2 /data/shard2_2 /data/shard3_2 /data/configmongo3mkdir -p /data/shard1_3 /data/shard2_3 /data/shard3_3 /data/config 3. mongo1配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_1/ --logpath /data/shard1_1/shard1_1.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_1/ --logpath /data/shard2_1/shard2_1.log --logappend --fork /usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_1/ --logpath /data/shard3_1/shard3_1.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 4. mongo2配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_2/ --logpath /data/shard1_2/shard1_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_2/ --logpath /data/shard2_2/shard2_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_2/ --logpath /data/shard3_2/shard3_2.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 5. mongo3配置副本集,启动mongos路由.123456789101112131415161718/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_3/ --logpath /data/shard1_3/shard1_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_3/ --logpath /data/shard2_3/shard2_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_3/ --logpath /data/shard3_3/shard3_3.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork --随便进入一台 27017 开启会议config=&#123;_id:&apos;shard1&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27017&apos;,priority:3&#125;, &#123;_id:1,host:&apos;mongo2:27017&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27017&apos;,priority:1&#125;]&#125;rs.initiate(config).--随便进入一台 27018 开启会议config=&#123;_id:&apos;shard2&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27018&apos;,priority:1&#125;, &#123;_id:1,host:&apos;mongo2:27018&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27018&apos;,priority:3&#125;]&#125;rs.initiate(config)--随便进入一台 27019 开启会议config=&#123;_id:&apos;shard3’,members:[ &#123;_id:0,host:&apos;mongo1:27019’,priority:2&#125;, &#123;_id:1,host:&apos;mongo2:27019’,priority:3&#125;,&#123;_id:2,host:&apos;mongo3:27019’,priority:1&#125;]&#125;rs.initiate(config) 6. 添加分片12345mongo --port 30000 随便进入一台use admindb.runCommand(&#123;addshard:&quot;shard1/mongo1:27017,mongo2:27017,mongo3:27017&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard2/mongo1:27018,mongo2:27018,mongo3:27018&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard3/mongo1:27019,mongo2:27019,mongo3:27019”&#125;) 7. test数据库开启Sharding1db.runCommand(&#123;enablesharding:&quot;test&quot;&#125;) 8. users 集合开启分片(要对一个集合分片,首先要对这个集合的数据库启用分片)1db.runCommand(&#123;shardcollection:&quot;test.users&quot;,key:&#123;_id:1&#125;&#125;)","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}]},{"title":"postGis 安装部署,对比geohash性能","date":"2016-12-09T05:47:14.000Z","path":"2016/12/09/postGis-安装部署-对比geohash性能/","text":"一、描述公司现获取最近楼栋信息用的是mongdb的geohash,想对比一下postgis和geohash的性能,首先假设我们安装好了PostgreSQL 9.5.2和Mongodb 3.0,mongodb的配置就不多少了,他本身集成geohash功能,安装配置也很简单,解压启动就OK了,postgis 相对比较麻烦,文献比较少,它依赖于很多插件(geos, proj,GDAL, libxml2, json-c,postgresql) 二、安装Postgis及插件1.安装geos12345tar -jxvf geos-3.5.0.tar.bz2cd geos-3.5.0./configuremakemake install 2.安装proj12345tar -zxvf proj.4-4.9.1.tar.gzcd proj.4-4.9.1./configuremakemake install 3.安装gdal12345tar -zxvf gdal-1.10.0.tar.gzcd gdal-1.10.0./configuremakemake install 4.安装libxml212345tar -zxvf libxml2-2.9.2.tar.gzcd libxml2-2.9.2./configuremakemake install 5. 安装json-c12345tar -zxvf json-c-json-c-0.11-20130402.tar.gzcd json-c-json-c-0.11-20130402./configuremakemake install 6. 安装postgis123456tar -zxvf postgis-2.1.8.tar.gzcd postgis-2.1.8# 这里要把前面装的插件全部配置上./configure --prefix=/usr/local/postgis --with-pgconfig=/usr/local/pgsql/bin/pg_config --with-xml2config=/usr/local/bin/xml2-config --with-geosconfig=/usr/local/bin/geos-config --with-gdalconfig=/usr/local/bin/gdal-configmakemake install 三、配置postgresql支持postgis1. postgresql 创建用户12create user luhuijun with password &apos;xxxxxx&apos;;grant all privileges on database customer to luhuijun; 2. 修改配置文件,因为postgresql的权限是文件/usr/local/pgsql/data/pg_hba.conf控制的,md5 密码认证,trust 免密码认证,这里跟mongodb又不一样,mongodb绑定的是自己的ip,内网,外网,回环(127.0.0.1),权限粒度大,内网段访问,外网段访问,本机访问, postgresql pg_hba.conf 配置文件host是访问者的ip,这个配置文件就能指定你只能访问那个数据库.加上role可以控制列的粒度,你只能访问一张表的某几个列. 3.把权限放大alter role luhuijun SUPERUSER; 4. 创建库,连接该数据库, 开启postgresql对postgis的支持1234create database custome;\\c customerCREATE EXTENSION postgis;CREATE EXTENSION postgis_topology; 现在就可以使用postgis 了. 四、查询脚本123 mongodb：db.runCommand(&#123;geoNear: &quot;gps_info&quot;, spherical: true, distanceMultiplier: 6378137, near: [120.6945, 27.998], num: 1, query: &#123;createdAt: &#123;$gt: &quot;2016-04-28&quot;&#125;&#125;&#125;);postgresql：select *,ST_Distance(jwd, ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163)) from building_gps_info order by jwd &lt;-&gt; ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163) limit 1; 五、Jmeter只读场景性能压测对比1. 在1核1GB的机器上,这是10个并发轮训100次对比结果,记录下来了平均响应时间,最小响应时间和最大响应时间.可以看到postgresql的平均响应时间比mongodb的平均响应时间快了28倍. 2. 在1核1GB的机器上,这是100个并发轮训100次得到的对比结果.可以看到伴随着并发量的增长,mongodb表现出来有些疲软,有些连接的最大响应时间达到了23.385秒,这应用应该已经慢到不能容忍了.并不像postgresql那么平稳. 六、 应用上的优势1. 酒后代驾,沿黄浦江画一条不规则的线,浦东的醉汉叫车时检索不到浦西的代驾司机,因为代驾司机的小电驴穿过江隧道比较吃力;2. 每30秒更新一次在线司机的经纬度信息，这是mongodb和mysql的myisam引擎做不到的，因为他们锁的最小粒度是表锁，更新的这段时间用户是无法下单的，能及时反馈你叫的司机离你有多远;3. 精准的分析，内环中环外环，某一片区域下了多少单，及时做推广;4. 业务员获取订单配送距离和推荐路线，需求点到点的距离计算、路径计算；5. 相似路径的多个订单的批量配送，需求位置和大量传统数据符合运算；6. 实时配送，位置跟踪。大量位置相关信息的存取，需要有较好的性能。七、把经纬度转换为Geo脚本–创建空间索引CREATE INDEX &quot;clean_cinema_data_idx&quot; ON &quot;public&quot;.&quot;clean_cinema_data&quot; USING gist(jwd); –动态修改经纬度脚本1select &apos;update clean_cinema_data set jwd=ST_GeomFromText(&apos;&apos;point(&apos;||longitude||&apos; &apos;||latitude||&apos;)&apos;&apos;,4326) where id=&apos;&apos;&apos;||ID||&apos;&apos;&apos;;&apos; from clean_cinema_data ; –查看效果select count(1) from clean_cinema_data where jwd is not null;","tags":[{"name":"postgis","slug":"postgis","permalink":"http://yoursite.com/tags/postgis/"}]},{"title":"Sqoop2 同步mysql 至 HDFS","date":"2016-12-08T02:14:53.000Z","path":"2016/12/08/Sqoop2-同步mysql-至-HDFS/","text":"1. 开启客户端sqoop2-shell 2. 配置sqoop server参数sqoop:000&gt; set server --host luhuijundeMacBook-Pro.local --port 12000 --webapp sqoop #luhuijundeMacBook-Pro.local 一般为HDFS主机名 –webapp官方文档说是指定的sqoop jetty服务器名称， 大概是一个自己能识别的用于标示这个服务器的名字吧。 3. 我们在使用的过程中可能会遇到错误，使用此配置打印错误sqoop:000&gt; set option --name verbose --value true 4. 验证是否已经连上sqoop:000&gt; show version --all #如果server version:能显示代表能正常连接 5. 使用help命令可以查看sqoop支持的所有命令1234567891011121314151617sqoop:000&gt; helpFor information about Sqoop, visit: http://sqoop.apache.org/Available commands: exit (\\x ) Exit the shell history (\\H ) Display, manage and recall edit-line history help (\\h ) Display this help message set (\\st ) Configure various client options and settings show (\\sh ) Display various objects and configuration options create (\\cr ) Create new object in Sqoop repository delete (\\d ) Delete existing object in Sqoop repository update (\\up ) Update objects in Sqoop repository clone (\\cl ) Create new object based on existing one start (\\sta) Start job stop (\\stp) Stop job status (\\stu) Display status of a job enable (\\en ) Enable object in Sqoop repository disable (\\di ) Disable object in Sqoop repository 6. 检查Sqoop server支持的连接sqoop:000&gt; show connector 7. 创建数据源头link12345678910111213141516sqoop:000&gt; create link -connector generic-jdbc-connectorName: First LinkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://mysql.server/databaseNameUsername:dbaPassword: *****Fetch Size:(回车)entry#protocol=tcpentry#(回车)Identifier enclose:(空格) #这里是指定SQL中标识符的定界符，也就是说，有的SQL标示符是一个引号：select * from &quot;table_name&quot;，这种定界符在MySQL中是会报错的。这个属性默认值就是双引号，所以不能使用回车，必须将之覆盖，我使用空格覆盖了这个值。官方文档这里有坑！New link was successfully created with validation status OK and name third link 看到这样的字符这个link算是创建成功 8. 创建目标link1234567891011sqoop:000&gt; create link -connector hdfs-connectorCreating link for connector with name hdfs-connectorPlease fill following values to create new link objectName: Second LinkLink configurationHDFS URI: hdfs://localhost:9000Conf directory:/usr/local/hadoop/etc/hadoopentry#(回车)New link was successfully created with validation status OK and name Second Link 看到这样的字符串代表创建成功 9. 使用两个link名字 from 和 to 来创建job12345678910111213141516171819202122232425262728293031323334353637383940414243444546sqoop:000&gt; create job -f &quot;First Link&quot; -t &quot;Second Link&quot; Creating job for links with from name First Link and to name Second Link Please fill following values to create new job object Name: Sqoopy #job 名称Schema name: test #mysql数据库名称Table name: test #表名称SQL statement:Column names:There are currently 0 values in the list:element#(回车)Partition column:(回车)Partition column nullable:(回车)Boundary query:(回车)Incremental readCheck column:(回车)Last value:(回车)Target configuration #配置目标Override null value: nullNull value: nullFile format: 0 : TEXT_FILE 1 : SEQUENCE_FILE 2 : PARQUET_FILEChoose: 0 #选择0最简单的文本文件Compression codec: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0 #选择0,不压缩Custom codec:(回车)Output directory: hdfs://localhost:9000/user/luhuijun/sqoop #最好是完全没有这个目录: sqoop,如果有目录里面又有文件, 又是一堆权限问题.Append mode:(回车)Throttling resourcesExtractors: 2Loaders: 2Classpath configurationExtra mapper jars:There are currently 0 values in the list:element#New job was successfully created with validation status OK and name Sqoopy 9. 开启job 并打印job执行详情12345678910111213141516sqoop:000&gt; start job -n Sqoopy -sSubmission detailsJob Name: SqoopyServer URL: http://luhuijundeMacBook-Pro.local:12000/sqoop/Created by: luhuijunCreation date: 2016-12-06 21:26:24 CSTLastly updated by: luhuijunExternal ID: job_1481030429951_0001 http://luhuijundeMacBook-Pro.local:8088/proxy/application_1481030429951_0001/Source Connector schema: Schema&#123;name= test . test ,columns=[ FixedPoint&#123;name=id,nullable=true,type=FIXED_POINT,byteSize=4,signed=true&#125;, Text&#123;name=name,nullable=true,type=TEXT,charSize=null&#125;]&#125;2016-12-06 21:26:24 CST: BOOTING - Progress is not available2016-12-06 21:26:36 CST: RUNNING - 0.00 %2016-12-06 21:26:46 CST: RUNNING - 50.00 %2016-12-06 21:26:57 CST: SUCCEEDED 10. 查看执行结果1234567➜ /usr/local/hadoop/etc/hadoop git:(master) &gt;hdfs dfs -cat &apos;sqoop/*&apos; #这是zsh *直接匹配不到,在正常的shell里应该不需要引号1,&apos;1&apos;2,&apos;xinle&apos;3,&apos;huijun&apos;4,&apos;hongna&apos; 11. 总结看上去简单的几步,其实踩了很多坑,在学习过程中通常会犯两类错误：第一 类错误是在知之不多的情况下就盲目开始,即行动太快；第二类错误是在行动之前准备过多,即行动太晚。要想在这 二者之间取得平衡，你掌握的知识要恰到好处，足以能让你开始学习， 但又不会多到让你无力探索，这样学习效果最佳。&lt;&lt;软技能 代码之外的生存指南&gt;&gt;","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]},{"title":"Sqoop2 version 1.99.7 安装部署","date":"2016-12-08T02:13:14.000Z","path":"2016/12/08/Sqoop2-version-1-99-7-安装部署/","text":"1. 版本对比Sqoop2 相比Sqoop1 升级幅度太大,可以说两个软件完全没有关系.Sqoop2 相比Sqoop1 增加了server端, sqoop1 是那种解压出来配置个环境变量就能直接使用的软件, sqoop2 安装部署使用复杂,而且官方给出来的文档有几个坑要踩踩. 2. Sqoop2 Version 1.99.7 安装部署下载软件:wget http://apache.fayea.com/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz 解压软件:tar -zvxf sqoop-1.99.7-bin-hadoop200.tar.gz 存放到指定路径:mv sqoop-1.99.7-bin-hadoop200 /usr/local/sqoop 赋权限:chmod -R 755 /usr/local/sqoop 修改环境变量:vim /etc/profile 最下方添加如下变量: 123456export SQOOP_HOME=/usr/local/sqoopexport PATH=$SQOOP_HOME/bin:$PATHexport CATALINA_BASE=$SQOOP_HOME/serverexport LOGDIR=$SQOOP_HOME/logsexport SQOOP_SERVER_EXTRA_LIB=/usr/local/sqoop/server/lib指定SQOOP_SERVER_EXTRA_LIB 后续要把连接mysql的jar包 copy 至这个文件加下 copy mysql-connector jar包至指定文件: cp /Users/luhuijun/Downloads/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $SQOOP_HOME/server/lib 修改配置文件:sudo vim /usr/local/sqoop/conf/sqoop.properties 修改指向我的hadoop配置文件目录:org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/usr/local/hadoop/etc/hadoop 配置catalina.properties 此文件不存在,需要自已建立:vim /usr/local/sqoop/conf/catalina.properties 内容如下: 1common.loader=/usr/local/hadoop/share/hadoop/common/*.jar,/usr/local/hadoop/share/hadoop/common/lib/*.jar,/usr/local/hadoop/share/hadoop/hdfs/*.jar,/usr/local/hadoop/share/hadoop/hdfs/lib/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/lib/*.jar,/usr/local/hadoop/share/hadoop/tools/*.jar,/usr/local/hadoop/share/hadoop/tools/lib/*.jar,/usr/local/hadoop/share/hadoop/yarn/*.jar,/usr/local/hadoop/share/hadoop/yarn/lib/*.jar,/usr/local/hadoop/share/hadoop/httpfs/tomcat/lib/*.jar, 修改启动脚本: 12345678vim /usr/local/sqoop/bin/sqoop.sh添加如下内容,注意自己的路径:export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home HADOOP_COMMON_HOME=/usr/local/hadoop/share/hadoop/common HADOOP_HDFS_HOME=/usr/local/hadoop/share/hadoop/hdfs HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce HADOOP_YARN_HOME=/usr/local/hadoop/share/hadoop/yarn 修改hadoop的yarn-site.xml: 1234567vim /usr/local/hadoop/etc/hadoop/yarn-site.xml添加如下属性:&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 修改hadoop的container-executor.cfg: 1234vim /usr/local/hadoop/etc/hadoop/container-executor.cfg修改如些配置:allowed.system.users=luhuijun 配置为当前用户名, 不然sqoop2-shell去访问hdfs 文件的时候会报错(user: luhuijun is not allowed to impersonate luhuijun)这个问题坑了我好久 修改core-site.xml: 1234567891011vim /usr/local/hadoop/etc/hadoop/core-site.xml添加如下配置:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.groups&lt;/name&gt; &lt;value&gt;staff&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.hosts&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; 跟修改上一个配置是解决同一个问题,sqoop2-shell去访问hdfs的时候会报(user: luhuijun is not allowed to impersonate luhuijun),staff 为当前系统用户的用户组, 使用groups能查到当前用户组名. 启动sqoop 服务:sqoop.sh server start 3. 验证启动是否成功12345sqoop.sh client 或 sqoop2-shell 进入客户端set server --host hadoopMaster --port 12000 --webapp sqoop 设置服务器，注意hadoopMaster为hdfs主机名show connector --all 查看连接类型show link 查看连接show job 查看job","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]}]