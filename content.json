[{"title":"Mycat自增主键设置","date":"2016-12-21T08:53:09.000Z","path":"2016/12/21/Mycat自增主键设置/","text":"在server.xml中，将sequnceHandlerType设置为11&lt;property name=&quot;sequnceHandlerType&quot;&gt;1&lt;/property&gt; 在schema.xml中，table中增加属性autoIncrement值为true，添加mycat_sequence表123456&lt;schema name=&quot;zhaimi&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;!-- random sharding using mod sharind rule --&gt; &lt;!-- autoIncrement=&quot;true&quot; 属性--&gt; &lt;table name=&quot;tt2&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;mycat_sequence&quot; primaryKey=&quot;name&quot; dataNode=&quot;dn1&quot; /&gt;&lt;/schema&gt; 在sequence_db_conf.properties中，依赖全局序列，增加序列，与table名称相同全大写TT2=dn20 创建mycat_sequence表,在dn20对应的数据库下面创建。1234567891011DROP TABLE IF EXISTS MYCAT_SEQUENCE; CREATE TABLE MYCAT_SEQUENCE( name VARCHAR(50) NOT NULL, current_value INT NOT NULL, increment INT NOT NULL DEFAULT 100, PRIMARY KEY(name) ) ENGINE=InnoDB; name：sequence名称（表明）currenct_value：当前value（当前值）increment：增长步长（增长步长）注：MYCAT_SEQUENCE必须大写 插入sequence记录1INSERT INTO MYCAT_SEQUENCE(name, current_value, increment) VALUES (TT2, 1, 100); 创建存储函数，必须在同一个数据库中创建，也就是在dn20对应的zhaimi库创建。12345678910111213141516171819202122232425262728293031323334353637383940获取当前sequence的值 (返回当前值,增量) DROP FUNCTION IF EXISTS mycat_seq_currval; DELIMITER ;; CREATE FUNCTION mycat_seq_currval(seq_name VARCHAR(50)) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN DECLARE retval VARCHAR(64); SET retval=&quot;-999999999,null&quot;; SELECT concat(CAST(current_value AS CHAR),&quot;,&quot;,CAST(increment AS CHAR)) INTO retval FROM MYCAT_SEQUENCE WHERE name = seq_name; RETURN retval; END ;;DELIMITER ; 设置sequence值 DROP FUNCTION IF EXISTS mycat_seq_setval; DELIMITER ;; CREATE FUNCTION mycat_seq_setval(seq_name VARCHAR(50),value INTEGER) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN UPDATE MYCAT_SEQUENCE SET current_value = value WHERE name = seq_name; RETURN mycat_seq_currval(seq_name); END ;; DELIMITER ; 获取下一个sequence值 DROP FUNCTION IF EXISTS mycat_seq_nextval; DELIMITER ;; CREATE FUNCTION mycat_seq_nextval(seq_name VARCHAR(50)) RETURNS varchar(64) CHARSET utf8 DETERMINISTIC BEGIN UPDATE MYCAT_SEQUENCE SET current_value = current_value + increment WHERE name = seq_name; RETURN mycat_seq_currval(seq_name); END ;;DELIMITER ;``` ### 在mysql中定义自增主键 CREATE TABLE tt2 ( id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, nm INT(10) UNSIGNED NOT NULL, PRIMARY KEY (id)) ENGINE=MYISAM AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;12### 插入记录 insert into tt2(nm) values (99);FROM:1.MyCAT自增字段和返回生成的主键ID的经验分享2.MyCAT 性能测试3.mycat分布式mysql中间件（自增主键）4.MyCAT全局序列号12### 批量整理配置文件和表的脚本 整理配置文件select CONCAT(upper(TABLE_NAME),’=dn20’) from TABLES where TABLE_SCHEMA=’zhaimi’; 整理表内容select CONCAT(‘insert into mycat_sequence(name,current_value,increment) values(“‘,upper(TABLE_NAME),’”,(select max(id) from ‘,upper(TABLE_NAME),’),100);’)from TABLES where TABLE_SCHEMA=’zhaimi’; 查看表内容SELECT * from mycat_sequence;12### 测试 desc zm_audits; 插入数据insert into zhaimi.zm_audits ( user_agent, operator_type, operand_id, params, operation_desc, created_at, updated_at, ip, operator_id) values ( ‘Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36’, ‘MERCHANT’, ‘301’, ‘{\\”scope\\”:\\”merchant\\”,\\”grant_type\\”:\\”password\\”,\\”name\\”:\\”ceshiceshi\\”,\\”password\\”:\\”ceshxxxx\\”}’, ‘LOGIN’, ‘2015-07-29 12:39:24’, ‘2015-07-29 12:39:24’, ‘3031268088’, ‘301’); 查看数据是否插入 select * from zhaimi.zm_audits where created_at=’2015-07-29 12:39:24’;```","tags":[{"name":"mycat","slug":"mycat","permalink":"http://yoursite.com/tags/mycat/"}]},{"title":"mycat 安装部署","date":"2016-12-21T08:38:17.000Z","path":"2016/12/21/mycat-安装部署/","text":"测试环境系统版本: Centos 6.5Mycat服务器ip: 10.47.121.213Mysql服务器ip: 主 10.168.163.172, 从 10.24.155.3Mysql版本: Mysql 5.6.28jkd版本:java version “1.8.0_66” 安装步骤启动12345678910tar -zvxf Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gzmv mycat /usr/local/vim /etc/profile环境变量内容如下:export MYCAT_HOME=/usr/local/mycatexport PATH=$PATH:$MYCAT_HOME/binsource /etc/profilemycat start 配置连接信息路径:$MYCAT_HOME/conf/schema.xml配置如下: 12345678910111213&lt;dataHost name=&quot;localhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;10.168.163.172:3306&quot; user=&quot;root&quot; password=&quot;Abc12345&quot;&gt; &lt;!-- can have multi read hosts --&gt; &lt;!-- &lt;readHost host=&quot;hostS2&quot; url=&quot;10.24.155.3:3306&quot; user=&quot;root&quot; password=&quot;xxx&quot; /&gt; --&gt; &lt;/writeHost&gt; &lt;writeHost host=&quot;hostS1&quot; url=&quot;10.24.155.3:3306&quot; user=&quot;root&quot; password=&quot;Abc12345&quot; /&gt; &lt;!-- &lt;writeHost host=&quot;hostM2&quot; url=&quot;localhost:3316&quot; user=&quot;root&quot; password=&quot;123456&quot;/&gt; --&gt;&lt;/dataHost&gt; 数据节点路径:$MYCAT_HOME/conf/schema.xml配置如下: 12345678910111213141516171819202122&lt;!-- 21个节点,其实很多表是不需要分片的,只有个别的几张大表而且不会超过20个 --&gt; &lt;dataNode name=&quot;dn0&quot; dataHost=&quot;localhost3&quot; database=&quot;order_000&quot; /&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost3&quot; database=&quot;order_001&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost3&quot; database=&quot;order_002&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost3&quot; database=&quot;order_003&quot; /&gt; &lt;dataNode name=&quot;dn4&quot; dataHost=&quot;localhost3&quot; database=&quot;order_004&quot; /&gt; &lt;dataNode name=&quot;dn5&quot; dataHost=&quot;localhost3&quot; database=&quot;order_005&quot; /&gt; &lt;dataNode name=&quot;dn6&quot; dataHost=&quot;localhost3&quot; database=&quot;order_006&quot; /&gt; &lt;dataNode name=&quot;dn7&quot; dataHost=&quot;localhost3&quot; database=&quot;order_007&quot; /&gt; &lt;dataNode name=&quot;dn8&quot; dataHost=&quot;localhost3&quot; database=&quot;order_008&quot; /&gt; &lt;dataNode name=&quot;dn9&quot; dataHost=&quot;localhost3&quot; database=&quot;order_009&quot; /&gt; &lt;dataNode name=&quot;dn10&quot; dataHost=&quot;localhost3&quot; database=&quot;order_010&quot; /&gt; &lt;dataNode name=&quot;dn11&quot; dataHost=&quot;localhost3&quot; database=&quot;order_011&quot; /&gt; &lt;dataNode name=&quot;dn12&quot; dataHost=&quot;localhost3&quot; database=&quot;order_012&quot; /&gt; &lt;dataNode name=&quot;dn13&quot; dataHost=&quot;localhost3&quot; database=&quot;order_013&quot; /&gt; &lt;dataNode name=&quot;dn14&quot; dataHost=&quot;localhost3&quot; database=&quot;order_014&quot; /&gt; &lt;dataNode name=&quot;dn15&quot; dataHost=&quot;localhost3&quot; database=&quot;order_015&quot; /&gt; &lt;dataNode name=&quot;dn16&quot; dataHost=&quot;localhost3&quot; database=&quot;order_016&quot; /&gt; &lt;dataNode name=&quot;dn17&quot; dataHost=&quot;localhost3&quot; database=&quot;order_017&quot; /&gt; &lt;dataNode name=&quot;dn18&quot; dataHost=&quot;localhost3&quot; database=&quot;order_018&quot; /&gt; &lt;dataNode name=&quot;dn19&quot; dataHost=&quot;localhost3&quot; database=&quot;order_019&quot; /&gt; &lt;dataNode name=&quot;dn20&quot; dataHost=&quot;localhost3&quot; database=&quot;zhaimi&quot; /&gt; 配置逻辑数据库路径:$MYCAT_HOME/conf/schema.xml配置如下: 1234567891011121314151617181920&lt;!-- 设置逻辑数据库名为zhaimi,以后连接mycat就是用这个数据库名--&gt; &lt;schema name=&quot;zhaimi&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;!-- autoIncrement=&quot;true&quot; 属性--&gt; &lt;table name=&quot;stores&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;activities&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;activities_new&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;addresses&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;announcement_receivers&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;announcements&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;apply_products&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;apply_store&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;table name=&quot;balances&quot; type=&quot;global&quot; primaryKey=&quot;id&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn20&quot; /&gt; &lt;!-- 需要分片表和对应的子表可以根据主外键关系配置--&gt; &lt;table name=&quot;orders&quot; primaryKey=&quot;ID&quot; dataNode=&quot;dn$0-19&quot; rule=&quot;mod-long&quot;&gt; &lt;childTable name=&quot;order_alterations&quot; joinKey=&quot;order_no&quot; parentKey=&quot;order_no&quot;/&gt; &lt;childTable name=&quot;order_details&quot; joinKey=&quot;order_no&quot; parentKey=&quot;order_no&quot;/&gt; &lt;childTable name=&quot;order_price&quot; joinKey=&quot;order_no&quot; parentKey=&quot;order_no&quot;/&gt; &lt;childTable name=&quot;order_promotions&quot; joinKey=&quot;order_no&quot; parentKey=&quot;order_no&quot;/&gt; &lt;/table&gt; &lt;/schema&gt; 上面的配置相当于创建了一个逻辑数据库和逻辑表,现在给这个逻辑数据库加上连接.路径:$MYCAT_HOME/conf/server.xml配置如下: 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;&lt;mycat:server xmlns:mycat=&quot;http://org.opencloudb/&quot;&gt; &lt;system&gt; &lt;property name=&quot;charset&quot;&gt;utf8&lt;/property&gt; &lt;property name=&quot;defaultSqlParser&quot;&gt;druidparser&lt;/property&gt; &lt;property name=&quot;sequnceHandlerType&quot;&gt;1&lt;/property&gt; &lt;/system&gt; &lt;user name=&quot;test&quot;&gt; &lt;property name=&quot;password&quot;&gt;test&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;zhaimi&lt;/property&gt; &lt;/user&gt;&lt;/mycat:server&gt; 分片规则配置路径:$MYCAT_HOME/conf/rule.xml配置如下: 12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mycat:rule SYSTEM &quot;rule.dtd&quot;&gt;&lt;mycat:rule xmlns:mycat=&quot;http://org.opencloudb/&quot;&gt;&lt;!-- 内容太多了,本例是按store_id 20 取模就只取相关内容吧.--&gt; &lt;tableRule name=&quot;mod-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;store_id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt; &lt;/tableRule&gt; &lt;!-- 东西不是一层不变的,你可以设置mod-id-long,mod-userid-long--&gt; &lt;tableRule name=&quot;mod-store-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt; &lt;/tableRule&gt; &lt;!-- 这样store_id mod 20 得0就写入dn0 对应order_000数据库,开发需要订单数据也不会再想以前那样检索20个数据库才能找到自己需要的数据.--&gt; &lt;function name=&quot;mod-long&quot; class=&quot;org.opencloudb.route.function.PartitionByMod&quot;&gt; &lt;!-- how many data nodes --&gt; &lt;property name=&quot;count&quot;&gt;20&lt;/property&gt; &lt;/function&gt; &lt;function name=&quot;mod-store-long&quot; class=&quot;org.opencloudb.route.function.PartitionByMod&quot;&gt; &lt;!-- how many data nodes --&gt; &lt;property name=&quot;count&quot;&gt;20&lt;/property&gt; &lt;/function&gt;&lt;/mycat:rule&gt; 测试连接1234567891011mysql -h127.0.0.1 -uroot -p123456 -P8066mysql&gt; show databases;+----------+| DATABASE |+----------+| zhaimi |+----------+1 row in set (0.00 sec)mysql&gt; use zhaimi可以开始查询自己需要的数据了 Mycat目前有哪些功能与特性.支持SQL 92标准.支持mysql集群,可以作为Proxy使用.支持JDBC连接多数据库.支持NOSQL 数据库.支持galera for mysql集群,percona-cluster或mariadb cluster,提供高可用性数据分片集群.自动故障切换,高可用性.支持读写分离,支持mysql双主多从,以及一主多从.支持全局表,数据库自动分片到多个节点,用于高效表关联查询,而且关联的时候没有2张表的限制..支持独有的E-R关系分片策略,实现高效的表关联查询.支持一致性Hash分片,有效解决分片扩容难题.多平台支持,部署和实施简单.支持Catelet开发,类似于数据库存储过程,用于跨分片浮渣SQL人工智能编码实现,143行Demo完成跨分片的两个表Join查询.支持NIO与AIO两种网络通信协议,windows下建议用AIO,Linux下建议用NIO.支持mysql存储过程调用.支持自增主键,支持Oracle的Sequence机制 Mycat除了mysql还支持哪些数据库mongodb,oracle,sqlserver,hive,db2,postgresql Mycat目前有生产案例么目前初步统计有600多家公司使用 Mycat稳定性与Cobar如何目前Mycat稳定性优于Cobar,而且一直在更新,Cobar已经停止维护,可以放心使用 mycat支持集群么目前mycat没有实施对多mycat集群的支持,可以暂时使用haproxy来做负载,或者统计硬件负载 mycat后台管理监控如何使用9066端口可以用JDBC方式执行命令,在界面上运行管理维护,也可以通过命令行查看,命令行操作是mysql -h127.0.0.1 -uroot -p123456 -P9066 mycat 主键插入后应用如何获取获得自增主键,插入记录后执行select last_insert_id()获取. Mycat如何启动与加入服务linux下:mycat start 启动mycat stop 停止mycat console 前台运行mycat restart 重启服务mycat pause 暂停mycat status 查看启动状态 Mycat 支持多表Join么mycat目前支持2个表join,后续会支持多表join,具体join请看mycat权威指南对应章节.","tags":[{"name":"mycat","slug":"mycat","permalink":"http://yoursite.com/tags/mycat/"}]},{"title":"Mysql5.6 GTID 半同步复制配置","date":"2016-12-15T08:52:42.000Z","path":"2016/12/15/Mysql5-6-GTID-半同步复制配置/","text":"半同步复制原理：确保至少一个slave将变更写到磁盘，也就是说，对于每一个连接来说如果发生master崩溃，之多丢失一个事务。一定要理解半同步复制不会阻塞事务的提交，而是知道事务写入至少一个slave中继日志猜想客户端发送响应。对于每一个连接，如果事务提交到存储引擎之后，发送到slave之前，发生系统崩溃，那么这个事务就会丢失，但是在slave确定事务提交之后才会想客户端发送确认，所以至多会丢失一个事务，通常一个客户端只会丢失一个事务。 半同步复制需要应对两种情况？ 如果所有的slave都崩溃了，无法确认事务是否写入中继日志了怎么办？如果master只连一个slave，这种情况不是不可能。 如果所有的slave连接都断开了怎么办？这时，master就无法将事务发出去。除了rpl-semi-sync-master-enable和rpl-semi-slave-enabled处理以上情况，还需要以下两个参数。1234567rpl-semi-sync-master-timeout=milliseconds --毫秒为了防止半同步复制收不到确认被阻塞，用此选项可以设置超时设置。如果master超时仍未收到任何确认，就还原为异步复制，不再使用半同步复制。rpl-semi-sync-master-wait-no-slave=&#123;on|off&#125;如果提交的事务但master没有任何连接的slave可用，master就无法将事务发出去，默认情况下master会等slave连接在超时限制内，然后确认事务已经写入磁盘。 监控半同步复制123456rpl_semi_sync_master_clients：这个状态变量提供master上的，支持并启动半同步复制的slave数量。rpl_semi_sync_master_status：表示master上半同步复制的状态，1表示活动状态，0表示非活动状态，如果未启用半同步复制，或则启用了半同步复制变成了异步复制，这个值为0rpm_semi_sync_slave_status：表示slave上的半同步复制，1表示活动状态，0表示非活动状态使用show status命令或者信息模式表GlOBAL_STATUS，能够查看这些变量的值，如果想把这些值左右其他用途，show status命令不好用，可以用信息模式表上执行select查看：select variable_value into @value from information_schema.GLOBAL_STATUS where Variable_name=‘Rpl_semi_sync_master_status’;不过这张表已经在mysql5.7中删除了。show variables like &quot;%semi%&quot;; 全局事务标示符1234从mysql5.6引入全局事务标识符，即每一个事务都有一个唯一的标示符，要使用的标示符是全局的，还要加上服务器的uuid，构成一对，如果服务器的uuid（由变量@@server_uuid可得）如果事务从master复制到slave，事务的二进制日志发生改变，因为slave需要将事务写入slave上的二进制文件，由于slave的配置可能与master的不同这个位置可能与master位置差别很大，但是全局事务标示符是一样的。注意guid被写入二进制日志，并且只会分配给已经写入二进制日志的事务，也就是说，如果关闭二进制日志，事务就不会分配gtid了，不管master还是slave都是这样，所以如果使用slave做障转移，需要开启二进制日志，如果没有开启二进制日志，slave就不会记录事务的GTID。 配置GTID 半同步复制 在master上安装半同步复制插件 123master&gt;INSTALL PLUGIN rpl_semi_sync_master SONAME &apos;semisync_master.so&apos;;master&gt;SET GLOBAL rpl_semi_sync_master_enabled=1;master&gt;SET GLOBAL rpl_semi_sync_master_timeout=1000; 在每个slave上安装slave插件 12slave&gt;INSTALL PLUGIN rpl_semi_sync_slave SONAME &apos;semisync_slave.so&apos;;slave&gt;SET GLOBAL rpl_semi_sync_slave_enabled=1; 所有的插件安装完成后，在master和slave上启用他们，通过两个服务器选项控制的同时也是选项实现，保证设置即使重启也继续生效。最好关闭服务 向maser的my.cnf文件添加选项 123456789101112131415[mysqld]server-id = 1log_bin = /var/log/mysql/mysql-bin.logmax_binlog_size = 1000Mbinlog-format = rowbinlog-do-db=testexpire-logs-days=7log_slave_updates = TRUE--半同步复制配置rpl_semi_sync_master_enabled = 1rpl_semi_sync_master_timeout = 1000 # 1 second--GTID配置，此行为注释内容gtid_mode=on --该选项生成全局事务的标示符，此行为注释内容enforce_gtid_consistency=on --该选项保证来自master的已执行的时间同样也会写入备用服务器的二进制日志，否则，备用服务器就无法变更间接发给slave，注意，默认选项是未启用的。master_info_repository=TABLE 向slave的my.cnf 文件添加选项 123456789101112log-bin= mysql-binbinlog_format=rowserver_id=11relay-log=slave-relay-binrelay-log-index=slave-relay-bin.indexskip_slave_start=1-- 半同步复制配置rpl_semi_sync_slave_enabled = 1-- GTID配置gtid_mode=onenforce_gtid_consistency=onlog_slave_updates = TRUE 重启master和slave的mysql服务 1234在master上执行service mysqld restart在slave上执行service mysqld restart 备份master上的test数据库，然后在slave上还原 123456789在maser上执行mysqldump -uUserName -pPassWord test&gt;/tmp/test.sql把备份文件copy至slavescp /tpm/test.sql slave:/tmp在slave上执行mysql -uUserName -pPassWord testmysql&gt; source /tmp/test.sql 在slave开启复制 12345678CHANGE MASTER TO MASTER_HOST=HOST NEW_MASTER,MASTER_PORT=PORT OF NEW_MASTER,MASTER_USER=REPLICATION USER NAME,MASTER_PASSWORD=REPLICATION_USER_PASSWORD,MASTER_AUTO_POSITION=1;start slave; 在slave上检查复制 1234567891011121314151617181920212223show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 10.168.163.172 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000005 Read_Master_Log_Pos: 151 Relay_Log_File: slave-relay-bin.000005 Relay_Log_Pos: 361 Relay_Master_Log_File: mysql-bin.000005 Slave_IO_Running: Yes Slave_SQL_Running: Yes Master_Server_Id: 1 Master_UUID: 8d9a8b0c-1ce1-11e6-8f30-00163e002282 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Auto_Position: 01 row in set (0.00 sec)","tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/tags/Mysql/"}]},{"title":"MySQL 误操作后如何快速恢复数据","date":"2016-12-14T09:41:01.000Z","path":"2016/12/14/MySQL-误操作后如何快速恢复数据/","text":"描述基本上每个跟数据库打交道的程序员（当然也可能是你同事）都会碰一个问题，MySQL误操作后如何快速回滚？比如，delete一张表，忘加限制条件，整张表没了。假如这还是线上环境核心业务数据，那这事就闹大了。误操作后，能快速回滚数据是非常重要的。 传统解法用全量备份重搭实例，再利用增量binlog备份，恢复到误操作之前的状态。然后跳过误操作的SQL，再继续应用binlog。此法费时费力，不值得再推荐。 利用binlog2sql快速闪回 首先，确认你的MySQL server开启了binlog，设置了以下参数:12345[mysqld]server-id = 1log_bin = /var/log/mysql/mysql-bin.logmax_binlog_size = 1000Mbinlog-format = row 如果没有开启binlog，也没有预先生成回滚SQL，那真的无法快速恢复数据了。对存放重要业务数据的MySQL，强烈建议开启binlog。 随后，安装开源工具binlog2sql。binlog2sql是一款简单易用的binlog解析工具，其中一个功能就是利用binlog进行闪回。 123条件python2.7以上,并且有安装pip-pythongit clone https://github.com/danfengcao/binlog2sql.gitpip install -r requirements.txt 然后，我们就可以生成回滚SQL了。 背景：误删了test库tb1表整张表的数据，需要紧急回滚。 test库tb1表原有数据123456789101112131415mysql&gt; select * from tb1;+------+--------+| id | name |+------+--------+| 2 | huijun || 1 | xinle |+------+--------+4 rows in set (0.00 sec) mysql&gt; delete from tb1;Query OK, 2 rows affected (0.00 sec) tb1表被清空mysql&gt; select * from tb1;Empty set (0.00 sec) 恢复数据步骤： 登录mysql，查看目前的binlog文件123456mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 12262268 |+------------------+-----------+ 最新的binlog文件是mysql-bin.000001，我们再定位误操作SQL的binlog位置1234$ python binlog2sql/binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p&apos;admin&apos; -dtest -ttb1 --start-file=&apos;mysql-bin.000001&apos;输出：DELETE FROM `test`.`tb1` WHERE `id`=1 AND `name`=&apos;xinle&apos; LIMIT 1; #start 1511 end 1690 time 2016-12-14 15:21:47DELETE FROM `test`.`tb1` WHERE `id`=2 AND `name`=&apos;huijun&apos; LIMIT 1; #start 1511 end 1690 time 2016-12-14 15:21:47 生成回滚sql，并检查回滚sql是否正确1234$ python binlog2sql/binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p&apos;admin&apos; -dtest -ttb1 --start-file=&apos;mysql-bin.000047&apos; --start-pos=1511 --end-pos=1690 -B输出：INSERT INTO `test`.`tb1`(`id`, `name`) VALUES (2, &apos;huijun&apos;); #start 1511 end 1690 time 2016-12-14 15:21:47INSERT INTO `test`.`tb1`(`id`, `name`) VALUES (1, &apos;xinle&apos;); #start 1511 end 1690 time 2016-12-14 15:21:47 确认回滚sql正确，执行回滚语句。登录mysql确认，数据回滚成功。123456789$ python binlog2sql.py -h127.0.0.1 -P3306 -uadmin -p&apos;admin&apos; -dtest -ttb1 --start-file=&apos;mysql-bin.000047&apos; --start-pos=1511 --end-pos=1690 -B | mysql -h127.0.0.1 -P3306 -uadmin -p&apos;admin&apos; mysql&gt; select * from tb1;+------+--------+| id | name |+------+--------+| 2 | huijun || 1 | xinle |+------+--------+ 至此，不用再担心被炒鱿鱼了。 常见问题有人会问，我DDL误操作了怎么快速回滚？比如drop了一张大表。 很难做到。因为即使在在row模式下，DDL操作也不会把每行数据的变化记录到binlog，所以DDL无法通过binlog回滚。实现DDL回滚，必须要在执行DDL前先备份老数据。确实有人通过修改mysql server源码实现了DDL的快速回滚，我找到阿里的xiaobin lin提交了一个patch。但据我所知，国内很少有互联网公司应用了这个特性。原因的话，我认为最主要还是懒的去折腾，没必要搞这个低频功能，次要原因是会增加一些额外存储。 所以，DDL误操作的话一般只能通过备份来恢复。如果公司连备份也不能用了，那真的建议去买张飞机票了。干啥？跑呗","tags":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/tags/Mysql/"}]},{"title":"Mac上安装单机版Spark","date":"2016-12-13T08:18:32.000Z","path":"2016/12/13/Mac上安装单机版Spark/","text":"下载安装测试下载地址:http://spark.apache.org/downloads.html 点击下载地址,现在最新版本是spark-2.0.1-bin-hadoop2.7.tgz ,大概180MB. 解压下载文件,并将这个文件放大你想要的位置. 然后这就算安装好了..mv spark-2.0.1-bin-hadoop /usr/local/ 测试 1234567891011121314151617181920--进入安装目录cd /usr/local/spark--启动./sbin/start-master.sh--打开浏览器输入,查看主机运行情况http://localhost:8080--加子嗣(现在主机是司令,要给他派点兵.)./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT--这是后台启动nohup spark-class org.apache.spark.deploy.worker.Worker spark:/192.168.70.145:7077 &amp;--回到浏览器http://localhost:8080在“Workers”列表下赫然出现了你的第一个子嗣。其状态State为ALIVE。表示它正在运作。这时候你需要记住，当前Terminal的窗口对应就是那个刚加入的Worker Id。删除子嗣工作完成后吧小崽子们放出去玩,只需要在哪个看似死机的teminal 里按下CTRL+C关闭主机./sbin/stop-master.sh WordCount脚本1234567891011进入spark-shell./bin/spark-shellimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.SparkContext._var conf=new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyApp&quot;)val sc=new SparkContext(conf)val input=sc.textFile(&quot;README.md&quot;) //这里可以设置为绝对路径val words=input.flatMap(line=&gt;line.split(&quot; &quot;))val counts=words.map(word=&gt;(word,1)).reduceByKey&#123;case (x,y)=&gt;x+y&#125;counts.saveAsTextFile(outputFile) //这里可以设置为绝对路径 在Intellij IDEA里执行WordCount12345678910111213141516171819import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConfimport org.scalatest._object SimpleApp &#123; def main(args: Array[String]) &#123; var logFile=&quot;/usr/local/spark/README.md&quot; var conf=new SparkConf().setAppName(&quot;Simple Application&quot;).setMaster(&quot;local&quot;) var sc=new SparkContext(conf) val data=Array(1,2,3,4,5) val distData=sc.parallelize(data) val sumArray=distData.reduce((a,b)=&gt;a+b) val logData=sc.textFile(logFile,2).cache() val numAs=logData.filter(line=&gt;line.contains(&quot;a&quot;)).count() val numBs=logData.filter(line=&gt;line.contains(&quot;b&quot;)).count() println(&quot;Lines with a:%s,Lines with b:%s&quot;.format(numAs,numBs)) println(&quot;sumArray is:&quot;+sumArray) &#125;&#125;","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"IntelliJ IDEA 消除Spark程序日志中INFO输出","date":"2016-12-12T16:01:05.000Z","path":"2016/12/13/IntelliJ-IDEA-消除Spark程序日志中INFO输出/","text":"说明在使用Intellij IDEA，local模式下运行Spark程序时，会在Run窗口打印出很多INFO信息，辅助信息太多可能会将有用的信息掩盖掉。如下所示 这里写图片描述123456789101112131415161718192021222324252627282930/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/bin/java -Didea.launcher.port=7532 &quot;-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin&quot; -Dfile.encoding=UTF-8 -classpath &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/tools.jar:/Users/luhuijun/mydoc/spark/target/classes:/Users/luhuijun/.m2/repository/org/scala-lang/scala-library/2.11.0/scala-library-2.11.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-core_2.11/2.0.1/spark-core_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/luhuijun/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/luhuijun/.m2/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/luhuijun/.m2/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/luhuijun/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/luhuijun/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/luhuijun/.m2/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/luhuijun/.m2/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-common/2.2.0/hadoop-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/luhuijun/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/luhuijun/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/luhuijun/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/luhuijun/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/luhuijun/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/luhuijun/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/luhuijun/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-auth/2.2.0/hadoop-auth-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.2.0/hadoop-hdfs-2.2.0.jar:/Users/luhuijun/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.2.0/hadoop-mapreduce-client-app-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.2.0/hadoop-mapreduce-client-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.2.0/hadoop-yarn-client-2.2.0.jar:/Users/luhuijun/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/Users/luhuijun/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/luhuijun/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.2.0/hadoop-yarn-server-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.2.0/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.2.0/hadoop-yarn-api-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.2.0/hadoop-mapreduce-client-core-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.2.0/hadoop-yarn-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.2.0/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-annotations/2.2.0/hadoop-annotations-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-launcher_2.11/2.0.1/spark-launcher_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-network-common_2.11/2.0.1/spark-network-common_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-network-shuffle_2.11/2.0.1/spark-network-shuffle_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-unsafe_2.11/2.0.1/spark-unsafe_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/luhuijun/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/luhuijun/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/luhuijun/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/luhuijun/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/Users/luhuijun/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/luhuijun/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/luhuijun/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/luhuijun/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/Users/luhuijun/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/luhuijun/.m2/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/luhuijun/.m2/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Users/luhuijun/.m2/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/luhuijun/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/Users/luhuijun/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/luhuijun/.m2/repository/org/apache/mesos/mesos/0.21.1/mesos-0.21.1-shaded-protobuf.jar:/Users/luhuijun/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar:/Users/luhuijun/.m2/repository/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar:/Users/luhuijun/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/luhuijun/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/luhuijun/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/luhuijun/.m2/repository/net/razorvine/pyrolite/4.9/pyrolite-4.9.jar:/Users/luhuijun/.m2/repository/net/sf/py4j/py4j/0.10.3/py4j-0.10.3.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-tags_2.11/2.0.1/spark-tags_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar:/Users/luhuijun/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/Users/luhuijun/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-sql_2.11/2.0.1/spark-sql_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/com/univocity/univocity-parsers/2.1.1/univocity-parsers-2.1.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-sketch_2.11/2.0.1/spark-sketch_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-catalyst_2.11/2.0.1/spark-catalyst_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/codehaus/janino/janino/2.7.8/janino-2.7.8.jar:/Users/luhuijun/.m2/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-column/1.7.0/parquet-column-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-common/1.7.0/parquet-common-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-encoding/1.7.0/parquet-encoding-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-generator/1.7.0/parquet-generator-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-hadoop/1.7.0/parquet-hadoop-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-format/2.3.0-incubating/parquet-format-2.3.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-jackson/1.7.0/parquet-jackson-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-hive_2.11/2.0.2/spark-hive_2.11-2.0.2.jar:/Users/luhuijun/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/Users/luhuijun/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/Users/luhuijun/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/luhuijun/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/luhuijun/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/luhuijun/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/Users/luhuijun/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/luhuijun/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/luhuijun/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/luhuijun/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/luhuijun/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/luhuijun/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/Users/luhuijun/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/Users/luhuijun/.m2/repository/org/json/json/20090211/json-20090211.jar:/Users/luhuijun/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/luhuijun/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/luhuijun/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/Users/luhuijun/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/luhuijun/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/luhuijun/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/luhuijun/.m2/repository/org/apache/derby/derby/10.10.2.0/derby-10.10.2.0.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/luhuijun/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/luhuijun/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/luhuijun/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/luhuijun/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/luhuijun/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/luhuijun/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/Users/luhuijun/.m2/repository/org/codehaus/janino/commons-compiler/2.7.6/commons-compiler-2.7.6.jar:/Users/luhuijun/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/Users/luhuijun/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/Users/luhuijun/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/luhuijun/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/Users/luhuijun/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/Users/luhuijun/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/luhuijun/.m2/repository/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar:/Users/luhuijun/.m2/repository/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar&quot; com.intellij.rt.execution.application.AppMain SparkSql16/12/12 23:46:59 INFO SparkContext: Running Spark version 2.0.116/12/12 23:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable16/12/12 23:47:04 INFO SecurityManager: Changing view acls to: luhuijun16/12/12 23:47:04 INFO SecurityManager: Changing modify acls to: luhuijun16/12/12 23:47:04 INFO SecurityManager: Changing view acls groups to: 16/12/12 23:47:04 INFO SecurityManager: Changing modify acls groups to: 16/12/12 23:47:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(luhuijun); groups with view permissions: Set(); users with modify permissions: Set(luhuijun); groups with modify permissions: Set()16/12/12 23:47:04 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 55294.16/12/12 23:47:04 INFO SparkEnv: Registering MapOutputTracker16/12/12 23:47:04 INFO SparkEnv: Registering BlockManagerMaster16/12/12 23:47:04 INFO DiskBlockManager: Created local directory at /private/var/folders/nm/3bv6gdms53115l22n2vhsypc0000gn/T/blockmgr-58f045b4-0e51-43fa-80dd-0af1c2482dc416/12/12 23:47:04 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB16/12/12 23:47:04 INFO SparkEnv: Registering OutputCommitCoordinator16/12/12 23:47:04 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.16/12/12 23:47:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.3.2:404016/12/12 23:47:04 INFO Executor: Starting executor ID driver on host localhost16/12/12 23:47:04 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 55295.16/12/12 23:47:08 INFO DAGScheduler: ResultStage 6 (show at SparkSql.scala:24) finished in 0.016 s16/12/12 23:47:08 INFO DAGScheduler: Job 4 finished: show at SparkSql.scala:24, took 0.019567 s16/12/12 23:47:08 INFO CodeGenerator: Code generated in 6.561389 ms+-------+| name|+-------+|Michael|| Andy|| Justin|+-------+结果隐藏在其中根本找不到... 解决方案要解决这个问题，主要是要正确设置好log4j文件，本文主要分析如何在local模式下，将Spark的INFO信息隐藏，不影响程序中的结果输出。1、在项目src路径下创建resources文件夹, 右击该文件Mark Directory as 选中Resources Root2、将spark根目录下的log4j.properties文件复制 到 src/resources文件夹下3、修改log4j.properties文件的内容 将第一行的log4j.rootCategory=INFO, console改成log4j.rootCategory=ERROR, console，只显示ERROR级别的日志。 测试结果再次运行该代码，可以看到INFO信息已经消失 这里写图片描述123456789101112131415161718192021/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/bin/java -Didea.launcher.port=7533 &quot;-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin&quot; -Dfile.encoding=UTF-8 -classpath &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/+------------+| value|+------------+|Name: Justin|+------------++------------+| value|+------------+|Name: Justin|+------------++---------------+----+| address|name|+---------------+----+|[Columbus,Ohio]| Yin|+---------------+----+Process finished with exit code 0","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"浅谈事务和锁","date":"2016-12-12T15:14:13.000Z","path":"2016/12/12/浅谈事务和锁/","text":"本篇文章使用SQL Server来描述原因: 是因为SQL Server默认支持悲观并发,并且乐观并发的种类比较多,Mysql和Postgresql直接默认就是乐观并发.感觉用SQL Server来讲会更加全面, 但是不好的是我早就没有SQL Server的IDE了,因为好几年没有接触过SQL Server数据库了. 原子性最小单位不可分割,要么全部提交,要么全部回滚.转账的例子:1234567begin transaction;从我账户扣钱update balance=balance-500 where id=&apos;my_id&apos;;向目标账户加钱update balance=balance+500 where id=&apos;target_id&apos;;commit;不存在扣了我的钱,目标账户没有加钱的情况. 一致性实现动态平衡,扣了我500元,加了他500元,但是我的总额不变. 隔离性事务之间相互隔离,不受影响。锁就跟此特性息息相关，不同隔离级别共享锁持有的时间是不一样的。排他锁持有时间就不说了，为了保证事务的一致性，无论什么隔离级别排他锁都是从事务的开始一直持续到事务结束。如果此语句没有加begin transcation就算一个隐式事务，也就是从语句开始执行持续到语句执行结束。 现在了解一下不同隔离级别对共享锁的影响。读未提交从语意上来讲，别人没有提交你能读，那么你读取的时候肯定是不加共享锁的，如果加了共享锁就违背了互斥原理 S排斥X，但是此时我们读到的数据为脏数据（既没有提交，也没有回滚的数据称为脏数据，是一个中间状态）。12345两种方法能读取到脏数据select * from TableName where id=1000 with(nolock);SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;select * from TableName where id=1000 读已提交也是SQL Server默认的隔离级别,所以SQL Server默认隔离级别为悲观并发，从语意上来讲，别人提交了你能读，但是共享锁是读取一条释放一条，读取一条释放一条，而不是持续到事务的结束。12345678begin transaction;读完就释放select * from TableName where id=1000;读完就释放select * from TableName where id=1000;所以第一次读取和第二次读取到的数据很有可能是不同的,如果第一次读完后，有其他事务修改了此行数据，那么第二次读取到的数据就会不一样。commit； 可重复读从语意上来讲,可以重复读取,也就是在同一个事务内不管读取多少次,读取到的值都是一样。那么这个隔离级别的共享锁一定是持续到事务结束的，这段时间内其他事务不能修改。123456789begin transaction;读完不释放select * from TableName where id=1000；等待20秒waitfor delay &apos;00:00:20&apos; 读完不释放select * from TableName where id=1000;直到执行commit 的时候共享锁才会释放, 所以每次读取到的数据是一样的.commit; 可序列化从语意上来讲,可以序列执行,他不但能持续到事务结束,还会在聚集键上上一个范围锁,range锁,这段时间内,不但不能修改这个范围的数据,就连插入都不可以。如果此表没有聚集索引，没有聚集键，那么这个共享锁是加在表上的，原因：SQL Server有堆表的概念，存放的直接是数据的物理位置（FileID：PageID：SlotNum）物理的东西没有范围的概念，磁盘上这一点数据，那一天数据很难给你上一个范围锁，所以表锁。 1234读完不释放,并且是key range锁,如果没有聚集索引是表锁select * from TableName where id&gt;1000 and id&lt;1005；读完不释放并且是key range锁,如果没有聚集索引是表锁select * from TableName where id&gt;1000 and id&lt;1005； 读已提交(快照)实现语句级别的读取一致性,读取的是行的老版本. 快照实现事务级别的读取一致性,读取的是行的老版本. 但是此隔离级别会照成更新冲突。冲突发生是因为事务2在Quantity值为324的时候开始，当这个值被事务1更新后，行版本324被存储到版本存储区内。事务2会在事务的持续时间内继续读取该行数据。如果两个更新操作都被允许成功执行的话，就会产生经典的更新丢失情形。事务1增加了200个数量，然后事务2会在初始值上增加300个数量并存储。由第一个事务增加的那200个产品就会彻底丢失，SQL Server不会允许这样的情况发生。 持久性事务一旦提交成功持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。即使出现了任何事故比如断电等，事务一旦提交，则持久化保存在数据库中.","tags":[{"name":"数据库原理","slug":"数据库原理","permalink":"http://yoursite.com/tags/数据库原理/"}]},{"title":"表设计短类型优势","date":"2016-12-12T09:03:43.000Z","path":"2016/12/12/表设计短类型优势/","text":"一、背景咱们业务数据库的表字段长度偏长，特别是varchar类型，全部都是varchar（255）；表现的比较慷慨，事实证明短类型有很大的优势。 二、详情描述使用varchar（5）和varchar（200）存放hello 有什么区别？变长类型对比使用varchar（5）和varchar（200）存放hello空间开销是一样的，但是短类型会有更大的优势；原因如下：更长的类型会消耗更多的内存，因为mysql会分配固定大小内存块来保存在内存中。更长的类型使用临时表进行排序的时候会特别糟糕，在利用磁盘临时表进行排序的时候同样糟糕。 如果数据库系统哪哪都有问题，先检查是不是内存问题？内存瓶颈会造成IO和CPU波动；内存应该算是数据库中最宝贵的资源，我们所有的架构设计，高性能方案；sharding分布式多点写入都是围绕着怎么才能不跟磁盘交互来完成业务逻辑实现；因为内存的速度和硬盘的速度不在同一个数量级，内存有压力的时候会把缓存的数据页清除；后续需要用到这部分数据时就会产生物理IO，速度大打折扣，同样内存有压力会清除执行计划。相同语句后续执行就需要重新编译，CPU压力也会有所增加。所以阿里云RDS租金都是跟内存成正比。更小的通常更快，因为他占用更小的磁盘，内存和CPU缓存，并且处理cpu周期也会更短，所以慷慨不是明智的选择。只分配真正需要的空间。 三、例子咱们系统里日期类型全都是datetime占用了8个字节，timestamp 不但有datetime的所有功能，一样的表现形式却只占用4个字节。通常这些列都是有索引的，那么timestamp类型的这个索引页存放的数量应该是datetime类型列的2倍，那么单次IO取的数据量应该是datetime的2倍，缓存命中率也将直线上升。","tags":[{"name":"Performance","slug":"Performance","permalink":"http://yoursite.com/tags/Performance/"}]},{"title":"解决爬虫数据(电影院比价系统)电影院名称不规范问题解决思路","date":"2016-12-12T08:20:38.000Z","path":"2016/12/12/解决爬虫数据-电影院比价系统-电影院名称不规范问题解决思路/","text":"1 问题各大网站录入电影院，地址没有统一的规范，造成电影票无法比价。 2 解决思路2.1 经纬度范围查找拿到数据中包含经度维度信息，根据经纬度范围查找锁定这些名字不同的电影院为同一家电影院。 2.1.1 各大网站使用的地图坐标协议不同（google、高德、腾讯、图吧地图、图吧导航）使用的是gcj02，百度、搜狗使用的是另外一种坐标协议bd09。所以网上找个java写的统一转换各大地图协议至百度地图的代码，然后改写为mysql的自定义函数，转换后误差在万分之五（距离大概是5-5.5米） 一、经纬度距离换算a）在纬度相等的情况下： 经度每隔0.00001度，距离相差约1米； 每隔0.0001度，距离相差约10米； 每隔0.001度，距离相差约100米； 每隔0.01度，距离相差约1000米； 每隔0.1度，距离相差约10000米。 b）在经度相等的情况下： 纬度每隔0.00001度，距离相差约1.1米； 每隔0.0001度，距离相差约11米； 每隔0.001度，距离相差约111米； 每隔0.01度，距离相差约1113米； 每隔0.1度，距离相差约11132米。 高德 convert to 百度经纬度函数（网上java有现成代码，这是根据java改写mysql代码）。各个地图经纬度转换 转换维度12345678910111213141516171819DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lat(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x*x+y*y)+ 0.00002 * sin(y*x_pi); SET theta=atan2(y,x)+ 0.000003 * cos(x*x_pi); SET longitude=z*cos(theta)+0.0065; SET latitude=z*sin(theta)+0.006; RETURN latitude;END |DELIMITER ; 测试SELECT convert_gcj02_to_bd09_lat(120.098703,29.324483); 转换经度123456789101112131415161718DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lng(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x * x + y * y) + 0.00002 * sin(y * x_pi); SET theta = atan2(y, x) + 0.000003 * cos(x * x_pi); SET longitude = z * cos(theta) + 0.0065; RETURN longitude;END |DELIMITER ; 测试SELECT convert_gcj02_to_bd09_lng(120.098703,29.324483); 根据经纬度计算距离函数1234567DELIMITER |CREATE FUNCTION `juli`(lat1 DOUBLE(10,7),lat2 DOUBLE(10,7),lng1 DOUBLE(10,7),lng2 DOUBLE(10,7)) RETURNS doubleBEGINSET @distance=round(6378.138*2*asin(sqrt(pow(sin( (lat1*pi()/180-lat2*pi()/180)/2),2)+cos(lat1*pi()/180)*cos(lat2*pi()/180)* pow(sin( (lng1*pi()/180-lng2*pi()/180)/2),2)))*1000);RETURN @distance;END |DELIMITER ; 弃用经纬度算法很多影院的经纬度信息为null，而且有些经纬度信息不太准确，所以后面弃用了根据经纬度去定位是否为同一家影院。 根据电影院名字，电话，地址的相识度匹配。公式如下count（相识单词之间A和B）/count(A)+count(B)-count(交集)) 代码如下：电影院名称相识度匹配 对比两个字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647DELIMITER ;;CREATE FUNCTION `levenshtein`( s1 TEXT, s2 TEXT) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, i, j, c, c_temp, cost INT; DECLARE s1_char CHAR; DECLARE cv0, cv1 TEXT; SET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0; IF s1 = s2 THEN RETURN 0; ELSEIF s1_len = 0 THEN RETURN s2_len; ELSEIF s2_len = 0 THEN RETURN s1_len; ELSE WHILE j &lt;= s2_len DO SET cv1 = CONCAT(cv1, UNHEX(HEX(j))); SET j = j + 1; END WHILE; WHILE i &lt;= s1_len DO SET s1_char = SUBSTRING(s1, i, 1); SET c = i; SET cv0 = UNHEX(HEX(i)); SET j = 1; WHILE j &lt;= s2_len DO SET c = c + 1; IF s1_char = SUBSTRING(s2, j, 1) THEN SET cost = 0; ELSE SET cost = 1; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j+1, 1)), 16, 10) + 1; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET cv0 = CONCAT(cv0, UNHEX(HEX(c))); SET j = j + 1; END WHILE; SET cv1 = cv0; SET i = i + 1; END WHILE; END IF; RETURN c; END ;;DELIMITER ; 两个字符串相识度占比1234567891011121314DELIMITER ;;CREATE FUNCTION `levenshtein_ratio`( s1 TEXT, s2 TEXT ) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, max_len INT; SET s1_len = LENGTH(s1), s2_len = LENGTH(s2); IF s1_len &gt; s2_len THEN SET max_len = s1_len; ELSE SET max_len = s2_len; END IF; RETURN ROUND((1 - LEVENSHTEIN(s1, s2) / max_len) * 100); END |DELIMITER ;; 通过几次测试，相识度大于等于90的大致为同一影院。个别电影院名字极度相仿的，可以对相识度值做一些调整。12SELECT *,levenshtein_ratio(&apos;龙海金逸影城(美一店)&apos;,cinema_name) xiangshi FROM `bidding_cinema_data`WHERE levenshtein_ratio(&apos;龙海金逸影城(美一店)&apos;,cinema_name)&gt;=90; 重回经纬度字符串匹配的精确度很难达到80以上(因为有的电影院名字很短,只有两个字或4个字)所以这些电影院相识度匹配的时候,很难区分… 问题采集到的数据,有的经纬度信息为null所以根据百度地图接口传入地址来补全经纬度信息. 根据经纬度范围打标签SQL脚本如下:打标签第一版本 123456789101112131415161718192021222324252627282930DELIMITER ;;CREATE PROCEDURE `set_lable`(lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id int,lables int)-- lng:维度-- lat:经度-- rounds:前后范围-- city_meta_id:城市编号-- labels:标签BEGIN set @lng=lng; set @lat=lat; set @rounds=rounds; set @lable=lables; set @city_meta_id=city_meta_id; update clean_cinema_data_copy as a inner join bidding_city_data as b on a.city_id=b.city_id and a.site_id=b.site_id SET lable=@lable, brand=replace(replace(replace(replace(replace(replace(replace(replace(cinema_name,&apos;电影院&apos;,&apos; &apos;),&apos;电影城&apos;,&apos; &apos;),&apos;影视城&apos;,&apos; &apos;),&apos;国际&apos;,&apos;&apos;),&apos;影院&apos;,&apos; &apos;),&apos;影城&apos;,&apos; &apos;),&apos;影视&apos;,&apos; &apos;),city_name,&apos; &apos;) where longitude&lt;&gt;0.0 and city_meta_id=@city_meta_id and latitude&gt;= @lat-@rounds and latitude&lt;@lat+@rounds and longitude&gt;= @lng-@rounds and longitude&lt;@lng+@rounds and lable is NULL; END;;DELIMITER ; 调用上面过程的脚本批量打标签第一版本 1234567set @rownum=0;selectconcat(&apos;call set_lable(&apos;,longitude,&apos;,&apos;,latitude,&apos;,&apos;,0.006,&apos;,&apos;,3120,&apos;,&apos;,@rownum:=@rownum+1,&apos;);&apos;)from clean_cinema_data_copywhere city_meta_id=3120and longitude&lt;&gt;0.0;-- 把此语句执行的结果复制到连接数据库的IDE里执行 根据经纬度范围打标签结果12345678910111213141516最终的准确度在65%-75% 之间, 距离最终90%还有一定距离. 所以后面会加上一些brand的词库. 根据经纬度范围打过标签之后再根据brand这个维度再打一次.上海市千分之六:大于等于4家的是128个, 等于4家的是91个;千分之三:大于等于4家的是139个, 等于4家的是113个;千分之二:大于等于4家的是142个, 等于4家的是125个; SELECT 125*1.0/165 准确度:0.75758; 北京市千分之六:大于等于4家的是110个, 等于4家的是83个; select 83*1.0/136 0.61029;千分之三:大于等于4家的是107个, 等于4家的是92个; select 92*1.0/136 准确度: 0.67647;千分之二:大于等于4家的是99个, 等于4家的是89个; SELECT 89*1.0/136 0.65441; 广州市千分之六:大于等于4家的是78个, 等于4家的是58个; select 58*1.0/105 0.55238;千分之三:大于等于4家的是79个, 等于4家的是68个; select 68*1.0/105 准确度:0.64762;千分之二:大于等于4家的是76个, 等于4家的是66个; SELECT 66*1.0/105 0.62857; 根据经纬度范围和词库brand两个维度打标签的准确率思路根据两个经纬度打标签,打完标签, 本来5个的6个的7个的可能会分出来1,2,3条,再加上一个维度打标签,完全为4个电影院的准确率为 上海市两个维度打标签 0.7879 上海由原来的0.75758 提升为0.75758 北京市两个维度打标签 0.7353 北京由原来的0.67647提升为0.7353 广州市两个维度打标签 0.6762 广州由原来的0.64762提升为0.6762准确率还是不太高…… 加维度逻辑如下:首先根据经纬度的范围打一次标签(把范围在200 米内的并且有4家[4个网站] 算为一个电影院) {结果集1}再把范围200米内不为4家(有的比较密集,8家,12家)加上简单的品牌分词, 按 机器标签, 品牌分组 等于4个的集合 {结果集2}再把城市所有数据 跟上面两个结果集的数据的交集求并集{结果集3}贪心算法 应用到结果集合3, 从500米开始步长循环处理每次递减50米…把完全等于4的集合放入临时表…(这里会产生9个临时表)创建过程把 集合1 U distinct {临时表1 U 临时表2 U 临时表 U 临时表3 U 临时表4 U 临时表5 U 临时表6 U 临时表7 U 临时表8 U 临时表9}取出来就是该城市最终数据, 上面公式中文解释 9个临时表取并集 去除重复后 跟集合1 取交集… 打标签代码如下打标签第二版本 12345678910111213141516171819202122232425262728293031323334DELIMITER ;;CREATE PROCEDURE `set_lable1`(tablename VARCHAR(48),lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id INT)BEGIN DECLARE a INT DEFAULT 1; SET @tablename=tablename; SET @lng=lng; SET @lat=lat; SET @rounds=rounds; SET @city_meta_id=city_meta_id; SET @v_sql=CONCAT(&apos;SELECT ifnull(max(lable)+1,1) INTO @nums FROM &apos;,@tablename); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt; SET @v_sql=CONCAT(&apos;UPDATE &apos;,@tablename,&apos; AS a INNER JOIN bidding_city_data AS b ON a.city_id=b.city_id AND a.site_id=b.site_id SET lable=&apos;,@nums,&apos;,&apos;, &apos; brand=LEFT(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(cinema_name,&quot;电影院&quot;,&quot;&quot;),&quot;电影城&quot;,&quot;&quot;),&quot;影视城&quot;,&quot;&quot;),&quot;国际&quot;,&quot;&quot;),&quot;影院&quot;,&quot;&quot;),&quot;影城&quot;,&quot;&quot;),&quot;影视&quot;,&quot;&quot;),city_name,&quot;&quot;),&quot;（&quot;,&quot;&quot;),&quot;(&quot;,&quot;&quot;),&quot;）&quot;,&quot;&quot;),&quot;)&quot;,&quot;&quot;),&quot;影剧院&quot;,&quot;&quot;),&quot;-&quot;,&quot;&quot;),&quot; &quot;,&quot;&quot;),3) WHERE longitude&lt;&gt;0.0 AND city_meta_id=&apos;,@city_meta_id,&apos; AND latitude&gt;=&apos;, @lat-@rounds, &apos; AND latitude&lt; &apos;,@lat+@rounds, &apos; AND longitude&gt;=&apos;, @lng-@rounds,&apos; AND longitude&lt; &apos;,@lng+@rounds, &apos; AND lable IS NULL;&apos;); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt;END;;DELIMITER ; 批量打标签脚本批量打标签第二版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299delimiter |CREATE PROCEDURE batch_set_lable1(city INT,rounds DOUBLE,groups INT)BEGIN DECLARE done INT DEFAULT -1; DECLARE lng DOUBLE; DECLARE lat DOUBLE; DECLARE cur CURSOR FOR SELECT longitude,latitude FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0.0; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done=1; OPEN cur; read_loop:LOOP FETCH cur INTO lng,lat; IF done=1 THEN LEAVE read_loop; END IF; CALL set_lable1(&apos;clean_cinema_data_copy&apos;,lng,lat,rounds,city); END LOOP; CLOSE cur; -- 根据经纬度范围0.002打标签和 -- 每组不等于4个的数据再根据品牌分组等于4的集合, -- 此集合与上海市的全部数据的差集, -- 是我们后续需要缩小经纬度分析的集合 DROP TABLE IF EXISTS step_5_0; CREATE TABLE step_5_0 AS SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy AS a INNER JOIN ( SELECT * FROM clean_cinema_data_copy WHERE city_meta_id=city GROUP BY `lable` HAVING count(1)=groups ) AS b ON a.`lable`=b.`lable` AND a.city_meta_id=city UNION SELECT c.* FROM clean_cinema_data_copy AS c INNER JOIN ( SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy a INNER JOIN ( SELECT lable, id, longitude, latitude, cinema_name, cinema_meta_id, brand, COUNT(DISTINCT cinema_meta_id) cinemas, COUNT(1) counts FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0 GROUP BY lable HAVING count(1)&lt;&gt;groups )b ON a.lable=b.lable AND a.brand=b.`brand` GROUP BY lable,brand HAVING count(1)=groups )tb GROUP BY lable,brand )d ON c.lable=d.`lable` AND c.brand=d.brand AND c.city_meta_id )h; 处理500米内的数据 SET @rownum=0; DROP TABLE IF EXISTS tmp_5_0; CREATE TABLE tmp_5_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_5_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_5_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_5_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_5_0&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理450米内的数据 DROP TABLE IF EXISTS tmp_4_5; CREATE TABLE tmp_4_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_5&apos;,lng,lat,0.0045,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理400米内的数据 DROP TABLE IF EXISTS tmp_4_0; CREATE TABLE tmp_4_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_0&apos;,lng,lat,0.004,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理350米内的数据 DROP TABLE IF EXISTS tmp_3_5; CREATE TABLE tmp_3_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_5&apos;,lng,lat,0.0035,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理300米内的数据 DROP TABLE IF EXISTS tmp_3_0; CREATE TABLE tmp_3_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_0&apos;,lng,lat,0.003,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理250米内的数据 DROP TABLE IF EXISTS tmp_2_5; CREATE TABLE tmp_2_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_5&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理200米内的数据 DROP TABLE IF EXISTS tmp_2_0; CREATE TABLE tmp_2_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_0&apos;,lng,lat,0.002,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理150米内的数据 DROP TABLE IF EXISTS tmp_1_5; CREATE TABLE tmp_1_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_5&apos;,lng,lat,0.0015,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理100米内的数据 DROP TABLE IF EXISTS tmp_1_0; CREATE TABLE tmp_1_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_0&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; 处理50米内的数据 DROP TABLE IF EXISTS tmp_0_5; CREATE TABLE tmp_0_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_0_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_0_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_0_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_0_5&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; END |DELIMITER ; 获取最终结果的过程获取结果的过程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697delimiter |CREATE PROCEDURE cinema_result(groups INT)begin SELECT id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name`,concat(&apos;step_&apos;,lable) lable FROM step_5_0 UNION SELECT * FROM ( SELECT DISTINCT id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name`,lable FROM ( SELECT * FROM tmp_5_0 WHERE lable IN ( SELECT lable FROM tmp_5_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_5 WHERE lable IN ( SELECT lable FROM tmp_4_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_0 WHERE lable IN ( SELECT lable FROM tmp_4_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_5 WHERE lable IN ( SELECT lable FROM tmp_3_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_0 WHERE lable IN ( SELECT lable FROM tmp_3_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_5 WHERE lable IN ( SELECT lable FROM tmp_2_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_0 WHERE lable IN ( SELECT lable FROM tmp_2_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_5 WHERE lable IN ( SELECT lable FROM tmp_1_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_0 WHERE lable IN ( SELECT lable FROM tmp_1_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_0_5 WHERE lable IN ( SELECT lable FROM tmp_0_5 GROUP BY lable HAVING count(1)=groups ) )tb )tb1 GROUP BY id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name` ORDER BY lable;END |delimiter ; 过程调用方法两个过程的使用方法/第一个参数是城市编号,第二个参数是第一次打标签使用的范围值(此处0.003或0.002能筛选出的数据最多).第三个参数:4 跟爬去的站点数对应 /CALL batch_set_lable1(3120,0.002,4); /参数的含义是分几个站,跟爬去的站点数量对应.从表中拿出最终结果. /CALL cinema_result(4); 用贪心算法得出北上广三个城市的正确率:上海:86.67%北京:85.29%广州:78.09% 剩余的一些数据:需要人工比对…根据贪心算法得出数据的准确率(此准确率是跟人工分组每组4个对比得出, 人工分组不等于4 小于4数据不完整,大于4此影院多拿一条数据我暂时认为数据非法, 哪怕机器分组 3 个,5个的跟人工的完全一致 , 也视为非法 )","tags":[{"name":"Work","slug":"Work","permalink":"http://yoursite.com/tags/Work/"}]},{"title":"MongoDB 分片","date":"2016-12-12T08:00:22.000Z","path":"2016/12/12/MongoDB-分片/","text":"加host1234vi /etc/hosts192.168.130.93 mongo1192.168.130.94 mongo2192.168.130.95 mongo3 创建目录123456mongo1mkdir -p /data/shard1_1 /data/shard2_1 /data/shard3_1 /data/configmongo2mkdir -p /data/shard1_2 /data/shard2_2 /data/shard3_2 /data/configmongo3mkdir -p /data/shard1_3 /data/shard2_3 /data/shard3_3 /data/config mongo1配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_1/ --logpath /data/shard1_1/shard1_1.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_1/ --logpath /data/shard2_1/shard2_1.log --logappend --fork /usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_1/ --logpath /data/shard3_1/shard3_1.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork mongo2配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_2/ --logpath /data/shard1_2/shard1_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_2/ --logpath /data/shard2_2/shard2_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_2/ --logpath /data/shard3_2/shard3_2.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork mongo3配置副本集,启动mongos路由.123456789101112131415161718/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_3/ --logpath /data/shard1_3/shard1_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_3/ --logpath /data/shard2_3/shard2_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_3/ --logpath /data/shard3_3/shard3_3.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork --随便进入一台 27017 开启会议config=&#123;_id:&apos;shard1&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27017&apos;,priority:3&#125;, &#123;_id:1,host:&apos;mongo2:27017&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27017&apos;,priority:1&#125;]&#125;rs.initiate(config).--随便进入一台 27018 开启会议config=&#123;_id:&apos;shard2&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27018&apos;,priority:1&#125;, &#123;_id:1,host:&apos;mongo2:27018&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27018&apos;,priority:3&#125;]&#125;rs.initiate(config)--随便进入一台 27019 开启会议config=&#123;_id:&apos;shard3’,members:[ &#123;_id:0,host:&apos;mongo1:27019’,priority:2&#125;, &#123;_id:1,host:&apos;mongo2:27019’,priority:3&#125;,&#123;_id:2,host:&apos;mongo3:27019’,priority:1&#125;]&#125;rs.initiate(config) 添加分片12345mongo --port 30000 随便进入一台use admindb.runCommand(&#123;addshard:&quot;shard1/mongo1:27017,mongo2:27017,mongo3:27017&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard2/mongo1:27018,mongo2:27018,mongo3:27018&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard3/mongo1:27019,mongo2:27019,mongo3:27019”&#125;) test数据库开启Sharding1db.runCommand(&#123;enablesharding:&quot;test&quot;&#125;) users 集合开启分片(要对一个集合分片,首先要对这个集合的数据库启用分片)1db.runCommand(&#123;shardcollection:&quot;test.users&quot;,key:&#123;_id:1&#125;&#125;)","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}]},{"title":"postGis 安装部署,对比geohash性能","date":"2016-12-09T05:47:14.000Z","path":"2016/12/09/postGis-安装部署-对比geohash性能/","text":"一、描述公司现获取最近楼栋信息用的是mongdb的geohash,想对比一下postgis和geohash的性能,首先假设我们安装好了PostgreSQL 9.5.2和Mongodb 3.0,mongodb的配置就不多少了,他本身集成geohash功能,安装配置也很简单,解压启动就OK了,postgis 相对比较麻烦,文献比较少,它依赖于很多插件(geos, proj,GDAL, libxml2, json-c,postgresql) 二、安装Postgis及插件安装geos12345tar -jxvf geos-3.5.0.tar.bz2cd geos-3.5.0./configuremakemake install 安装proj12345tar -zxvf proj.4-4.9.1.tar.gzcd proj.4-4.9.1./configuremakemake install 安装gdal12345tar -zxvf gdal-1.10.0.tar.gzcd gdal-1.10.0./configuremakemake install 安装libxml212345tar -zxvf libxml2-2.9.2.tar.gzcd libxml2-2.9.2./configuremakemake install 安装json-c12345tar -zxvf json-c-json-c-0.11-20130402.tar.gzcd json-c-json-c-0.11-20130402./configuremakemake install 安装postgis123456tar -zxvf postgis-2.1.8.tar.gzcd postgis-2.1.8这里要把前面装的插件全部配置上./configure --prefix=/usr/local/postgis --with-pgconfig=/usr/local/pgsql/bin/pg_config --with-xml2config=/usr/local/bin/xml2-config --with-geosconfig=/usr/local/bin/geos-config --with-gdalconfig=/usr/local/bin/gdal-configmakemake install 三、配置postgresql支持postgispostgresql 创建用户12create user luhuijun with password &apos;xxxxxx&apos;;grant all privileges on database customer to luhuijun; 修改配置文件,因为postgresql的权限是文件/usr/local/pgsql/data/pg_hba.conf控制的,md5 密码认证,trust 免密码认证,这里跟mongodb又不一样,mongodb绑定的是自己的ip,内网,外网,回环(127.0.0.1),权限粒度大,内网段访问,外网段访问,本机访问, postgresql pg_hba.conf 配置文件host是访问者的ip,这个配置文件就能指定你只能访问那个数据库.加上role可以控制列的粒度,你只能访问一张表的某几个列. 把权限放大alter role luhuijun SUPERUSER; 开启postgresql对postgis的支持1234create database custome;\\c customerCREATE EXTENSION postgis;CREATE EXTENSION postgis_topology; 现在就可以使用postgis 了. 查询脚本123 mongodb：db.runCommand(&#123;geoNear: &quot;gps_info&quot;, spherical: true, distanceMultiplier: 6378137, near: [120.6945, 27.998], num: 1, query: &#123;createdAt: &#123;$gt: &quot;2016-04-28&quot;&#125;&#125;&#125;);postgresql：select *,ST_Distance(jwd, ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163)) from building_gps_info order by jwd &lt;-&gt; ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163) limit 1; Jmeter只读场景性能压测对比在1核1GB的机器上,这是10个并发轮训100次对比结果,记录下来了平均响应时间,最小响应时间和最大响应时间.可以看到postgresql的平均响应时间比mongodb的平均响应时间快了28倍.在1核1GB的机器上,这是100个并发轮训100次得到的对比结果.可以看到伴随着并发量的增长,mongodb表现出来有些疲软,有些连接的最大响应时间达到了23.385秒,这应用应该已经慢到不能容忍了.并不像postgresql那么平稳. 应用上的优势酒后代驾,沿黄浦江画一条不规则的线,浦东的醉汉叫车时检索不到浦西的代驾司机,因为代驾司机的小电驴穿过江隧道比较吃力;每30秒更新一次在线司机的经纬度信息，这是mongodb和mysql的myisam引擎做不到的，因为他们锁的最小粒度是表锁，更新的这段时间用户是无法下单的，能及时反馈你叫的司机离你有多远;精准的分析，内环中环外环，某一片区域下了多少单，及时做推广;业务员获取订单配送距离和推荐路线，需求点到点的距离计算、路径计算；相似路径的多个订单的批量配送，需求位置和大量传统数据符合运算；实时配送，位置跟踪。大量位置相关信息的存取，需要有较好的性能。 把经纬度转换为Geo脚本创建空间索引CREATE INDEX &quot;clean_cinema_data_idx&quot; ON &quot;public&quot;.&quot;clean_cinema_data&quot; USING gist(jwd); –动态修改经纬度脚本1select &apos;update clean_cinema_data set jwd=ST_GeomFromText(&apos;&apos;point(&apos;||longitude||&apos; &apos;||latitude||&apos;)&apos;&apos;,4326) where id=&apos;&apos;&apos;||ID||&apos;&apos;&apos;;&apos; from clean_cinema_data ; 查看效果select count(1) from clean_cinema_data where jwd is not null;","tags":[{"name":"postgis","slug":"postgis","permalink":"http://yoursite.com/tags/postgis/"}]},{"title":"Sqoop2 同步mysql 至 HDFS","date":"2016-12-08T02:14:53.000Z","path":"2016/12/08/Sqoop2-同步mysql-至-HDFS/","text":"开启客户端sqoop2-shell 配置sqoop server参数sqoop:000&gt; set server --host luhuijundeMacBook-Pro.local --port 12000 --webapp sqoop #luhuijundeMacBook-Pro.local 一般为HDFS主机名 –webapp官方文档说是指定的sqoop jetty服务器名称， 大概是一个自己能识别的用于标示这个服务器的名字吧。 我们在使用的过程中可能会遇到错误，使用此配置打印错误sqoop:000&gt; set option --name verbose --value true 验证是否已经连上sqoop:000&gt; show version --all #如果server version:能显示代表能正常连接 使用help命令可以查看sqoop支持的所有命令1234567891011121314151617sqoop:000&gt; helpFor information about Sqoop, visit: http://sqoop.apache.org/Available commands: exit (\\x ) Exit the shell history (\\H ) Display, manage and recall edit-line history help (\\h ) Display this help message set (\\st ) Configure various client options and settings show (\\sh ) Display various objects and configuration options create (\\cr ) Create new object in Sqoop repository delete (\\d ) Delete existing object in Sqoop repository update (\\up ) Update objects in Sqoop repository clone (\\cl ) Create new object based on existing one start (\\sta) Start job stop (\\stp) Stop job status (\\stu) Display status of a job enable (\\en ) Enable object in Sqoop repository disable (\\di ) Disable object in Sqoop repository 检查Sqoop server支持的连接sqoop:000&gt; show connector 创建数据源头link12345678910111213141516sqoop:000&gt; create link -connector generic-jdbc-connectorName: First LinkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://mysql.server/databaseNameUsername:dbaPassword: *****Fetch Size:(回车)entry#protocol=tcpentry#(回车)Identifier enclose:(空格) #这里是指定SQL中标识符的定界符，也就是说，有的SQL标示符是一个引号：select * from &quot;table_name&quot;，这种定界符在MySQL中是会报错的。这个属性默认值就是双引号，所以不能使用回车，必须将之覆盖，我使用空格覆盖了这个值。官方文档这里有坑！New link was successfully created with validation status OK and name third link 看到这样的字符这个link算是创建成功 创建目标link1234567891011sqoop:000&gt; create link -connector hdfs-connectorCreating link for connector with name hdfs-connectorPlease fill following values to create new link objectName: Second LinkLink configurationHDFS URI: hdfs://localhost:9000Conf directory:/usr/local/hadoop/etc/hadoopentry#(回车)New link was successfully created with validation status OK and name Second Link 看到这样的字符串代表创建成功 使用两个link名字 from 和 to 来创建job12345678910111213141516171819202122232425262728293031323334353637383940414243444546sqoop:000&gt; create job -f &quot;First Link&quot; -t &quot;Second Link&quot; Creating job for links with from name First Link and to name Second Link Please fill following values to create new job object Name: Sqoopy #job 名称Schema name: test #mysql数据库名称Table name: test #表名称SQL statement:Column names:There are currently 0 values in the list:element#(回车)Partition column:(回车)Partition column nullable:(回车)Boundary query:(回车)Incremental readCheck column:(回车)Last value:(回车)Target configuration #配置目标Override null value: nullNull value: nullFile format: 0 : TEXT_FILE 1 : SEQUENCE_FILE 2 : PARQUET_FILEChoose: 0 #选择0最简单的文本文件Compression codec: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0 #选择0,不压缩Custom codec:(回车)Output directory: hdfs://localhost:9000/user/luhuijun/sqoop #最好是完全没有这个目录: sqoop,如果有目录里面又有文件, 又是一堆权限问题.Append mode:(回车)Throttling resourcesExtractors: 2Loaders: 2Classpath configurationExtra mapper jars:There are currently 0 values in the list:element#New job was successfully created with validation status OK and name Sqoopy 开启job 并打印job执行详情12345678910111213141516sqoop:000&gt; start job -n Sqoopy -sSubmission detailsJob Name: SqoopyServer URL: http://luhuijundeMacBook-Pro.local:12000/sqoop/Created by: luhuijunCreation date: 2016-12-06 21:26:24 CSTLastly updated by: luhuijunExternal ID: job_1481030429951_0001 http://luhuijundeMacBook-Pro.local:8088/proxy/application_1481030429951_0001/Source Connector schema: Schema&#123;name= test . test ,columns=[ FixedPoint&#123;name=id,nullable=true,type=FIXED_POINT,byteSize=4,signed=true&#125;, Text&#123;name=name,nullable=true,type=TEXT,charSize=null&#125;]&#125;2016-12-06 21:26:24 CST: BOOTING - Progress is not available2016-12-06 21:26:36 CST: RUNNING - 0.00 %2016-12-06 21:26:46 CST: RUNNING - 50.00 %2016-12-06 21:26:57 CST: SUCCEEDED 查看执行结果1234567➜ /usr/local/hadoop/etc/hadoop git:(master) &gt;hdfs dfs -cat &apos;sqoop/*&apos; #这是zsh *直接匹配不到,在正常的shell里应该不需要引号1,&apos;1&apos;2,&apos;xinle&apos;3,&apos;huijun&apos;4,&apos;hongna&apos; 总结看上去简单的几步,其实踩了很多坑,在学习过程中通常会犯两类错误：第一 类错误是在知之不多的情况下就盲目开始,即行动太快；第二类错误是在行动之前准备过多,即行动太晚。要想在这 二者之间取得平衡，你掌握的知识要恰到好处，足以能让你开始学习， 但又不会多到让你无力探索，这样学习效果最佳。&lt;&lt;软技能 代码之外的生存指南&gt;&gt;","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]},{"title":"Sqoop2 version 1.99.7 安装部署","date":"2016-12-08T02:13:14.000Z","path":"2016/12/08/Sqoop2-version-1-99-7-安装部署/","text":"版本对比Sqoop2 相比Sqoop1 升级幅度太大,可以说两个软件完全没有关系.Sqoop2 相比Sqoop1 增加了server端, sqoop1 是那种解压出来配置个环境变量就能直接使用的软件, sqoop2 安装部署使用复杂,而且官方给出来的文档有几个坑要踩踩. Sqoop2 Version 1.99.7 安装部署下载软件:wget http://apache.fayea.com/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz 解压软件:tar -zvxf sqoop-1.99.7-bin-hadoop200.tar.gz 存放到指定路径:mv sqoop-1.99.7-bin-hadoop200 /usr/local/sqoop 赋权限:chmod -R 755 /usr/local/sqoop 修改环境变量:vim /etc/profile 最下方添加如下变量: 123456export SQOOP_HOME=/usr/local/sqoopexport PATH=$SQOOP_HOME/bin:$PATHexport CATALINA_BASE=$SQOOP_HOME/serverexport LOGDIR=$SQOOP_HOME/logsexport SQOOP_SERVER_EXTRA_LIB=/usr/local/sqoop/server/lib指定SQOOP_SERVER_EXTRA_LIB 后续要把连接mysql的jar包 copy 至这个文件加下 copy mysql-connector jar包至指定文件: cp /Users/luhuijun/Downloads/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $SQOOP_HOME/server/lib 修改配置文件:sudo vim /usr/local/sqoop/conf/sqoop.properties 修改指向我的hadoop配置文件目录:org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/usr/local/hadoop/etc/hadoop 配置catalina.properties 此文件不存在,需要自已建立:vim /usr/local/sqoop/conf/catalina.properties 内容如下: 1common.loader=/usr/local/hadoop/share/hadoop/common/*.jar,/usr/local/hadoop/share/hadoop/common/lib/*.jar,/usr/local/hadoop/share/hadoop/hdfs/*.jar,/usr/local/hadoop/share/hadoop/hdfs/lib/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/lib/*.jar,/usr/local/hadoop/share/hadoop/tools/*.jar,/usr/local/hadoop/share/hadoop/tools/lib/*.jar,/usr/local/hadoop/share/hadoop/yarn/*.jar,/usr/local/hadoop/share/hadoop/yarn/lib/*.jar,/usr/local/hadoop/share/hadoop/httpfs/tomcat/lib/*.jar, 修改启动脚本: 12345678vim /usr/local/sqoop/bin/sqoop.sh添加如下内容,注意自己的路径:export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home HADOOP_COMMON_HOME=/usr/local/hadoop/share/hadoop/common HADOOP_HDFS_HOME=/usr/local/hadoop/share/hadoop/hdfs HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce HADOOP_YARN_HOME=/usr/local/hadoop/share/hadoop/yarn 修改hadoop的yarn-site.xml: 1234567vim /usr/local/hadoop/etc/hadoop/yarn-site.xml添加如下属性:&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 修改hadoop的container-executor.cfg: 1234vim /usr/local/hadoop/etc/hadoop/container-executor.cfg修改如些配置:allowed.system.users=luhuijun 配置为当前用户名, 不然sqoop2-shell去访问hdfs 文件的时候会报错(user: luhuijun is not allowed to impersonate luhuijun)这个问题坑了我好久 修改core-site.xml: 1234567891011vim /usr/local/hadoop/etc/hadoop/core-site.xml添加如下配置:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.groups&lt;/name&gt; &lt;value&gt;staff&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.hosts&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; 跟修改上一个配置是解决同一个问题,sqoop2-shell去访问hdfs的时候会报(user: luhuijun is not allowed to impersonate luhuijun),staff 为当前系统用户的用户组, 使用groups能查到当前用户组名. 启动sqoop 服务:sqoop.sh server start 验证启动是否成功12345sqoop.sh client 或 sqoop2-shell 进入客户端set server --host hadoopMaster --port 12000 --webapp sqoop 设置服务器，注意hadoopMaster为hdfs主机名show connector --all 查看连接类型show link 查看连接show job 查看job","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]}]