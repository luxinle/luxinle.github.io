[{"title":"解决爬虫数据(电影院比价系统)电影院名称不规范问题解决思路","date":"2016-12-12T08:20:38.000Z","path":"2016/12/12/解决爬虫数据-电影院比价系统-电影院名称不规范问题解决思路/","text":"1 问题各大网站录入电影院，地址没有统一的规范，造成电影票无法比价。 2 解决思路2.1 经纬度范围查找拿到数据中包含经度维度信息，根据经纬度范围查找锁定这些名字不同的电影院为同一家电影院。 2.1.1 各大网站使用的地图坐标协议不同（google、高德、腾讯、图吧地图、图吧导航）使用的是gcj02，百度、搜狗使用的是另外一种坐标协议bd09。所以网上找个java写的统一转换各大地图协议至百度地图的代码，然后改写为mysql的自定义函数，转换后误差在万分之五（距离大概是5-5.5米） 一、经纬度距离换算a）在纬度相等的情况下： 经度每隔0.00001度，距离相差约1米； 每隔0.0001度，距离相差约10米； 每隔0.001度，距离相差约100米； 每隔0.01度，距离相差约1000米； 每隔0.1度，距离相差约10000米。 b）在经度相等的情况下： 纬度每隔0.00001度，距离相差约1.1米； 每隔0.0001度，距离相差约11米； 每隔0.001度，距离相差约111米； 每隔0.01度，距离相差约1113米； 每隔0.1度，距离相差约11132米。 – 高德 convert to 百度经纬度函数（网上java有现成代码，这是根据java改写mysql代码）。各个地图经纬度转换 – 转换维度12345678910111213141516171819DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lat(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x*x+y*y)+ 0.00002 * sin(y*x_pi); SET theta=atan2(y,x)+ 0.000003 * cos(x*x_pi); SET longitude=z*cos(theta)+0.0065; SET latitude=z*sin(theta)+0.006; RETURN latitude;END |DELIMITER ; – 测试SELECT convert_gcj02_to_bd09_lat(120.098703,29.324483); – 转换经度123456789101112131415161718DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lng(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x * x + y * y) + 0.00002 * sin(y * x_pi); SET theta = atan2(y, x) + 0.000003 * cos(x * x_pi); SET longitude = z * cos(theta) + 0.0065; RETURN longitude;END |DELIMITER ; – 测试SELECT convert_gcj02_to_bd09_lng(120.098703,29.324483); – 根据经纬度计算距离函数1234567DELIMITER |CREATE FUNCTION `juli`(lat1 DOUBLE(10,7),lat2 DOUBLE(10,7),lng1 DOUBLE(10,7),lng2 DOUBLE(10,7)) RETURNS doubleBEGINSET @distance=round(6378.138*2*asin(sqrt(pow(sin( (lat1*pi()/180-lat2*pi()/180)/2),2)+cos(lat1*pi()/180)*cos(lat2*pi()/180)* pow(sin( (lng1*pi()/180-lng2*pi()/180)/2),2)))*1000);RETURN @distance;END |DELIMITER ; 2.1.2 弃用经纬度算法很多影院的经纬度信息为null，而且有些经纬度信息不太准确，所以后面弃用了根据经纬度去定位是否为同一家影院。 3. 根据电影院名字，电话，地址的相识度匹配。3.1 公式如下count（相识单词之间A和B）/count(A)+count(B)-count(交集)) 3.2 代码如下：电影院名称相识度匹配 – 对比两个字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950DELIMITER ;;CREATE FUNCTION `levenshtein`( s1 TEXT, s2 TEXT) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, i, j, c, c_temp, cost INT; DECLARE s1_char CHAR; DECLARE cv0, cv1 TEXT; SET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0; IF s1 = s2 THEN RETURN 0; ELSEIF s1_len = 0 THEN RETURN s2_len; ELSEIF s2_len = 0 THEN RETURN s1_len; ELSE WHILE j &lt;= s2_len DO SET cv1 = CONCAT(cv1, UNHEX(HEX(j))); SET j = j + 1; END WHILE; WHILE i &lt;= s1_len DO SET s1_char = SUBSTRING(s1, i, 1); SET c = i; SET cv0 = UNHEX(HEX(i)); SET j = 1; WHILE j &lt;= s2_len DO SET c = c + 1; IF s1_char = SUBSTRING(s2, j, 1) THEN SET cost = 0; ELSE SET cost = 1; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j+1, 1)), 16, 10) + 1; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET cv0 = CONCAT(cv0, UNHEX(HEX(c))); SET j = j + 1; END WHILE; SET cv1 = cv0; SET i = i + 1; END WHILE; END IF; RETURN c; END ;;DELIMITER ;``` #### -- 两个字符串相识度占比 DELIMITER ;;CREATE FUNCTION levenshtein_ratio( s1 TEXT, s2 TEXT ) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, max_len INT; SET s1_len = LENGTH(s1), s2_len = LENGTH(s2); IF s1_len &gt; s2_len THEN SET max_len = s1_len; ELSE SET max_len = s2_len; END IF; RETURN ROUND((1 - LEVENSHTEIN(s1, s2) / max_len) * 100); END |DELIMITER ;;12#### -- 通过几次测试，相识度大于等于90的大致为同一影院。个别电影院名字极度相仿的，可以对相识度值做一些调整。 SELECT *,levenshtein_ratio(‘龙海金逸影城(美一店)’,cinema_name) xiangshi FROM bidding_cinema_dataWHERE levenshtein_ratio(‘龙海金逸影城(美一店)’,cinema_name)&gt;=90;12345678910## 4. 重回经纬度字符串匹配的精确度很难达到80以上(因为有的电影院名字很短,只有两个字或4个字) 所以这些电影院相识度匹配的时候,很难区分...### 4.1 问题采集到的数据,有的经纬度信息为null所以根据百度地图接口传入地址来补全经纬度信息.### 4.2 根据经纬度范围打标签SQL脚本如下:打标签第一版本 DELIMITER ;;CREATE PROCEDURE set_lable(lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id int,lables int) – lng:维度– lat:经度– rounds:前后范围– city_meta_id:城市编号– labels:标签 BEGIN set @lng=lng; set @lat=lat; set @rounds=rounds; set @lable=lables; set @city_meta_id=city_meta_id; update clean_cinema_data_copy as a inner join bidding_city_data as b on a.city_id=b.city_id and a.site_id=b.site_id SET lable=@lable, brand=replace(replace(replace(replace(replace(replace(replace(replace(cinema_name,&apos;电影院&apos;,&apos; &apos;),&apos;电影城&apos;,&apos; &apos;),&apos;影视城&apos;,&apos; &apos;),&apos;国际&apos;,&apos;&apos;),&apos;影院&apos;,&apos; &apos;),&apos;影城&apos;,&apos; &apos;),&apos;影视&apos;,&apos; &apos;),city_name,&apos; &apos;) where longitude&lt;&gt;0.0 and city_meta_id=@city_meta_id and latitude&gt;= @lat-@rounds and latitude&lt;@lat+@rounds and longitude&gt;= @lng-@rounds and longitude&lt;@lng+@rounds and lable is NULL; END;;DELIMITER ;123### 4.3 调用上面过程的脚本批量打标签第一版本 set @rownum=0;selectconcat(‘call set_lable(‘,longitude,’,’,latitude,’,’,0.006,’,’,3120,’,’,@rownum:=@rownum+1,’);’)from clean_cinema_data_copywhere city_meta_id=3120and longitude&lt;&gt;0.0;– 把此语句执行的结果复制到连接数据库的IDE里执行1### 4.4 根据经纬度范围打标签结果 最终的准确度在65%-75% 之间, 距离最终90%还有一定距离.所以后面会加上一些brand的词库. 根据经纬度范围打过标签之后再根据brand这个维度再打一次.上海市千分之六:大于等于4家的是128个, 等于4家的是91个;千分之三:大于等于4家的是139个, 等于4家的是113个;千分之二:大于等于4家的是142个, 等于4家的是125个; SELECT 125*1.0/165 准确度:0.75758; 北京市千分之六:大于等于4家的是110个, 等于4家的是83个; select 831.0/136 0.61029;千分之三:大于等于4家的是107个, 等于4家的是92个; select 921.0/136 准确度: 0.67647;千分之二:大于等于4家的是99个, 等于4家的是89个; SELECT 89*1.0/136 0.65441; 广州市千分之六:大于等于4家的是78个, 等于4家的是58个; select 581.0/105 0.55238;千分之三:大于等于4家的是79个, 等于4家的是68个; select 681.0/105 准确度:0.64762;千分之二:大于等于4家的是76个, 等于4家的是66个; SELECT 66*1.0/105 0.62857;12345678910111213141516171819202122## 5. 根据经纬度范围和词库brand两个维度打标签的准确率思路根据两个经纬度打标签,打完标签, 本来5个的6个的7个的可能会分出来1,2,3条,再加上一个维度打标签,完全为4个电影院的准确率为 -- 上海市两个维度打标签 0.7879 上海由原来的0.75758 提升为0.75758-- 北京市两个维度打标签 0.7353 北京由原来的0.67647提升为0.7353-- 广州市两个维度打标签 0.6762 广州由原来的0.64762提升为0.6762准确率还是不太高......## 6. 加维度逻辑如下: #### 6.1 首先根据经纬度的范围打一次标签(把范围在200 米内的并且有4家[4个网站] 算为一个电影院) &#123;结果集1&#125;#### 6.2 再把范围200米内不为4家(有的比较密集,8家,12家)加上简单的品牌分词, 按 机器标签, 品牌分组 等于4个的集合 &#123;结果集2&#125;#### 6.3 再把城市所有数据 跟上面两个结果集的数据的交集求并集&#123;结果集3&#125;#### 6.4 贪心算法 应用到结果集合3, 从500米开始步长循环处理每次递减50米...把完全等于4的集合放入临时表...(这里会产生9个临时表)#### 6.5 创建过程把 集合1 U distinct &#123;临时表1 U 临时表2 U 临时表 U 临时表3 U 临时表4 U 临时表5 U 临时表6 U 临时表7 U 临时表8 U 临时表9&#125; 取出来就是该城市最终数据, 上面公式中文解释 9个临时表取并集 去除重复后 跟集合1 取交集... ## 7. 打标签代码如下:打标签第二版本 DELIMITER ;;CREATE PROCEDURE set_lable1(tablename VARCHAR(48),lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id INT)BEGIN DECLARE a INT DEFAULT 1; SET @tablename=tablename; SET @lng=lng; SET @lat=lat; SET @rounds=rounds; SET @city_meta_id=city_meta_id; SET @v_sql=CONCAT(‘SELECT ifnull(max(lable)+1,1) INTO @nums FROM ‘,@tablename); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt; SET @v_sql=CONCAT(&apos;UPDATE &apos;,@tablename,&apos; AS a INNER JOIN bidding_city_data AS b ON a.city_id=b.city_id AND a.site_id=b.site_id SET lable=&apos;,@nums,&apos;,&apos;, &apos; brand=LEFT(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(cinema_name,&quot;电影院&quot;,&quot;&quot;),&quot;电影城&quot;,&quot;&quot;),&quot;影视城&quot;,&quot;&quot;),&quot;国际&quot;,&quot;&quot;),&quot;影院&quot;,&quot;&quot;),&quot;影城&quot;,&quot;&quot;),&quot;影视&quot;,&quot;&quot;),city_name,&quot;&quot;),&quot;（&quot;,&quot;&quot;),&quot;(&quot;,&quot;&quot;),&quot;）&quot;,&quot;&quot;),&quot;)&quot;,&quot;&quot;),&quot;影剧院&quot;,&quot;&quot;),&quot;-&quot;,&quot;&quot;),&quot; &quot;,&quot;&quot;),3) WHERE longitude&lt;&gt;0.0 AND city_meta_id=&apos;,@city_meta_id,&apos; AND latitude&gt;=&apos;, @lat-@rounds, &apos; AND latitude&lt; &apos;,@lat+@rounds, &apos; AND longitude&gt;=&apos;, @lng-@rounds,&apos; AND longitude&lt; &apos;,@lng+@rounds, &apos; AND lable IS NULL;&apos;); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt; END;;DELIMITER ;123## 8. 批量打标签脚本批量打标签第二版本 delimiter |CREATE PROCEDURE batch_set_lable1(city INT,rounds DOUBLE,groups INT)BEGIN DECLARE done INT DEFAULT -1; DECLARE lng DOUBLE; DECLARE lat DOUBLE; DECLARE cur CURSOR FOR SELECT longitude,latitude FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0.0; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done=1; OPEN cur; read_loop:LOOP FETCH cur INTO lng,lat; IF done=1 THEN LEAVE read_loop; END IF; CALL set_lable1(‘clean_cinema_data_copy’,lng,lat,rounds,city); END LOOP; CLOSE cur; -- 根据经纬度范围0.002打标签和 -- 每组不等于4个的数据再根据品牌分组等于4的集合, -- 此集合与上海市的全部数据的差集, -- 是我们后续需要缩小经纬度分析的集合 DROP TABLE IF EXISTS step_5_0; CREATE TABLE step_5_0 AS SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy AS a INNER JOIN ( SELECT * FROM clean_cinema_data_copy WHERE city_meta_id=city GROUP BY `lable` HAVING count(1)=groups ) AS b ON a.`lable`=b.`lable` AND a.city_meta_id=city UNION SELECT c.* FROM clean_cinema_data_copy AS c INNER JOIN ( SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy a INNER JOIN ( SELECT lable, id, longitude, latitude, cinema_name, cinema_meta_id, brand, COUNT(DISTINCT cinema_meta_id) cinemas, COUNT(1) counts FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0 GROUP BY lable HAVING count(1)&lt;&gt;groups )b ON a.lable=b.lable AND a.brand=b.`brand` GROUP BY lable,brand HAVING count(1)=groups )tb GROUP BY lable,brand )d ON c.lable=d.`lable` AND c.brand=d.brand AND c.city_meta_id )h; -- 处理500米内的数据 SET @rownum=0; DROP TABLE IF EXISTS tmp_5_0; CREATE TABLE tmp_5_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_5_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_5_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_5_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_5_0&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理450米内的数据 DROP TABLE IF EXISTS tmp_4_5; CREATE TABLE tmp_4_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_5&apos;,lng,lat,0.0045,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理400米内的数据 DROP TABLE IF EXISTS tmp_4_0; CREATE TABLE tmp_4_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_0&apos;,lng,lat,0.004,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理350米内的数据 DROP TABLE IF EXISTS tmp_3_5; CREATE TABLE tmp_3_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_5&apos;,lng,lat,0.0035,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理300米内的数据 DROP TABLE IF EXISTS tmp_3_0; CREATE TABLE tmp_3_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_0&apos;,lng,lat,0.003,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理250米内的数据 DROP TABLE IF EXISTS tmp_2_5; CREATE TABLE tmp_2_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_5&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理200米内的数据 DROP TABLE IF EXISTS tmp_2_0; CREATE TABLE tmp_2_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_0&apos;,lng,lat,0.002,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理150米内的数据 DROP TABLE IF EXISTS tmp_1_5; CREATE TABLE tmp_1_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_5&apos;,lng,lat,0.0015,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理100米内的数据 DROP TABLE IF EXISTS tmp_1_0; CREATE TABLE tmp_1_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_0&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理50米内的数据 DROP TABLE IF EXISTS tmp_0_5; CREATE TABLE tmp_0_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_0_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_0_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_0_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_0_5&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; END |DELIMITER ;123## 9. 获取最终结果的过程获取结果的过程 delimiter |CREATE PROCEDURE cinema_result(groups INT)begin SELECT id,cinema_id,agent_id,cinema_name,area,addr,area_name, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,cinema_meta_id,unique_name,concat(‘step_’,lable) lable FROM step_5_0 UNION SELECT * FROM ( SELECT DISTINCT id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name`,lable FROM ( SELECT * FROM tmp_5_0 WHERE lable IN ( SELECT lable FROM tmp_5_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_5 WHERE lable IN ( SELECT lable FROM tmp_4_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_0 WHERE lable IN ( SELECT lable FROM tmp_4_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_5 WHERE lable IN ( SELECT lable FROM tmp_3_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_0 WHERE lable IN ( SELECT lable FROM tmp_3_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_5 WHERE lable IN ( SELECT lable FROM tmp_2_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_0 WHERE lable IN ( SELECT lable FROM tmp_2_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_5 WHERE lable IN ( SELECT lable FROM tmp_1_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_0 WHERE lable IN ( SELECT lable FROM tmp_1_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_0_5 WHERE lable IN ( SELECT lable FROM tmp_0_5 GROUP BY lable HAVING count(1)=groups ) )tb )tb1 GROUP BY id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name` ORDER BY lable; END |delimiter ;``` 10. 过程调用方法两个过程的使用方法/第一个参数是城市编号,第二个参数是第一次打标签使用的范围值(此处0.003或0.002能筛选出的数据最多).第三个参数:4 跟爬去的站点数对应 /CALL batch_set_lable1(3120,0.002,4); /参数的含义是分几个站,跟爬去的站点数量对应.从表中拿出最终结果. /CALL cinema_result(4); 用贪心算法得出北上广三个城市的正确率:上海:86.67%北京:85.29%广州:78.09% 剩余的一些数据:需要人工比对…11. 根据贪心算法得出数据的准确率(此准确率是跟人工分组每组4个对比得出, 人工分组不等于4 小于4数据不完整,大于4此影院多拿一条数据我暂时认为数据非法, 哪怕机器分组 3 个,5个的跟人工的完全一致 , 也视为非法 )","tags":[{"name":"Work","slug":"Work","permalink":"http://yoursite.com/tags/Work/"}]},{"title":"MongoDB 分片","date":"2016-12-12T08:00:22.000Z","path":"2016/12/12/MongoDB-分片/","text":"1. 加host1234vi /etc/hosts192.168.130.93 mongo1192.168.130.94 mongo2192.168.130.95 mongo3 2. 创建目录123456mongo1mkdir -p /data/shard1_1 /data/shard2_1 /data/shard3_1 /data/configmongo2mkdir -p /data/shard1_2 /data/shard2_2 /data/shard3_2 /data/configmongo3mkdir -p /data/shard1_3 /data/shard2_3 /data/shard3_3 /data/config 3. mongo1配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_1/ --logpath /data/shard1_1/shard1_1.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_1/ --logpath /data/shard2_1/shard2_1.log --logappend --fork /usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_1/ --logpath /data/shard3_1/shard3_1.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 4. mongo2配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_2/ --logpath /data/shard1_2/shard1_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_2/ --logpath /data/shard2_2/shard2_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_2/ --logpath /data/shard3_2/shard3_2.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 5. mongo3配置副本集,启动mongos路由.123456789101112131415161718/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_3/ --logpath /data/shard1_3/shard1_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_3/ --logpath /data/shard2_3/shard2_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_3/ --logpath /data/shard3_3/shard3_3.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork --随便进入一台 27017 开启会议config=&#123;_id:&apos;shard1&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27017&apos;,priority:3&#125;, &#123;_id:1,host:&apos;mongo2:27017&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27017&apos;,priority:1&#125;]&#125;rs.initiate(config).--随便进入一台 27018 开启会议config=&#123;_id:&apos;shard2&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27018&apos;,priority:1&#125;, &#123;_id:1,host:&apos;mongo2:27018&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27018&apos;,priority:3&#125;]&#125;rs.initiate(config)--随便进入一台 27019 开启会议config=&#123;_id:&apos;shard3’,members:[ &#123;_id:0,host:&apos;mongo1:27019’,priority:2&#125;, &#123;_id:1,host:&apos;mongo2:27019’,priority:3&#125;,&#123;_id:2,host:&apos;mongo3:27019’,priority:1&#125;]&#125;rs.initiate(config) 6. 添加分片12345mongo --port 30000 随便进入一台use admindb.runCommand(&#123;addshard:&quot;shard1/mongo1:27017,mongo2:27017,mongo3:27017&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard2/mongo1:27018,mongo2:27018,mongo3:27018&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard3/mongo1:27019,mongo2:27019,mongo3:27019”&#125;) 7. test数据库开启Sharding1db.runCommand(&#123;enablesharding:&quot;test&quot;&#125;) 8. users 集合开启分片(要对一个集合分片,首先要对这个集合的数据库启用分片)1db.runCommand(&#123;shardcollection:&quot;test.users&quot;,key:&#123;_id:1&#125;&#125;)","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}]},{"title":"postGis 安装部署,对比geohash性能","date":"2016-12-09T05:47:14.000Z","path":"2016/12/09/postGis-安装部署-对比geohash性能/","text":"一、描述公司现获取最近楼栋信息用的是mongdb的geohash,想对比一下postgis和geohash的性能,首先假设我们安装好了PostgreSQL 9.5.2和Mongodb 3.0,mongodb的配置就不多少了,他本身集成geohash功能,安装配置也很简单,解压启动就OK了,postgis 相对比较麻烦,文献比较少,它依赖于很多插件(geos, proj,GDAL, libxml2, json-c,postgresql) 二、安装Postgis及插件1.安装geos12345tar -jxvf geos-3.5.0.tar.bz2cd geos-3.5.0./configuremakemake install 2.安装proj12345tar -zxvf proj.4-4.9.1.tar.gzcd proj.4-4.9.1./configuremakemake install 3.安装gdal12345tar -zxvf gdal-1.10.0.tar.gzcd gdal-1.10.0./configuremakemake install 4.安装libxml212345tar -zxvf libxml2-2.9.2.tar.gzcd libxml2-2.9.2./configuremakemake install 5. 安装json-c12345tar -zxvf json-c-json-c-0.11-20130402.tar.gzcd json-c-json-c-0.11-20130402./configuremakemake install 6. 安装postgis123456tar -zxvf postgis-2.1.8.tar.gzcd postgis-2.1.8# 这里要把前面装的插件全部配置上./configure --prefix=/usr/local/postgis --with-pgconfig=/usr/local/pgsql/bin/pg_config --with-xml2config=/usr/local/bin/xml2-config --with-geosconfig=/usr/local/bin/geos-config --with-gdalconfig=/usr/local/bin/gdal-configmakemake install 三、配置postgresql支持postgis1. postgresql 创建用户12create user luhuijun with password &apos;xxxxxx&apos;;grant all privileges on database customer to luhuijun; 2. 修改配置文件,因为postgresql的权限是文件/usr/local/pgsql/data/pg_hba.conf控制的,md5 密码认证,trust 免密码认证,这里跟mongodb又不一样,mongodb绑定的是自己的ip,内网,外网,回环(127.0.0.1),权限粒度大,内网段访问,外网段访问,本机访问, postgresql pg_hba.conf 配置文件host是访问者的ip,这个配置文件就能指定你只能访问那个数据库.加上role可以控制列的粒度,你只能访问一张表的某几个列. 3.把权限放大alter role luhuijun SUPERUSER; 4. 创建库,连接该数据库, 开启postgresql对postgis的支持1234create database custome;\\c customerCREATE EXTENSION postgis;CREATE EXTENSION postgis_topology; 现在就可以使用postgis 了. 四、查询脚本123 mongodb：db.runCommand(&#123;geoNear: &quot;gps_info&quot;, spherical: true, distanceMultiplier: 6378137, near: [120.6945, 27.998], num: 1, query: &#123;createdAt: &#123;$gt: &quot;2016-04-28&quot;&#125;&#125;&#125;);postgresql：select *,ST_Distance(jwd, ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163)) from building_gps_info order by jwd &lt;-&gt; ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163) limit 1; 五、Jmeter只读场景性能压测对比1. 在1核1GB的机器上,这是10个并发轮训100次对比结果,记录下来了平均响应时间,最小响应时间和最大响应时间.可以看到postgresql的平均响应时间比mongodb的平均响应时间快了28倍. 2. 在1核1GB的机器上,这是100个并发轮训100次得到的对比结果.可以看到伴随着并发量的增长,mongodb表现出来有些疲软,有些连接的最大响应时间达到了23.385秒,这应用应该已经慢到不能容忍了.并不像postgresql那么平稳. 六、 应用上的优势1. 酒后代驾,沿黄浦江画一条不规则的线,浦东的醉汉叫车时检索不到浦西的代驾司机,因为代驾司机的小电驴穿过江隧道比较吃力;2. 每30秒更新一次在线司机的经纬度信息，这是mongodb和mysql的myisam引擎做不到的，因为他们锁的最小粒度是表锁，更新的这段时间用户是无法下单的，能及时反馈你叫的司机离你有多远;3. 精准的分析，内环中环外环，某一片区域下了多少单，及时做推广;4. 业务员获取订单配送距离和推荐路线，需求点到点的距离计算、路径计算；5. 相似路径的多个订单的批量配送，需求位置和大量传统数据符合运算；6. 实时配送，位置跟踪。大量位置相关信息的存取，需要有较好的性能。七、把经纬度转换为Geo脚本–创建空间索引CREATE INDEX &quot;clean_cinema_data_idx&quot; ON &quot;public&quot;.&quot;clean_cinema_data&quot; USING gist(jwd); –动态修改经纬度脚本1select &apos;update clean_cinema_data set jwd=ST_GeomFromText(&apos;&apos;point(&apos;||longitude||&apos; &apos;||latitude||&apos;)&apos;&apos;,4326) where id=&apos;&apos;&apos;||ID||&apos;&apos;&apos;;&apos; from clean_cinema_data ; –查看效果select count(1) from clean_cinema_data where jwd is not null;","tags":[{"name":"postgis","slug":"postgis","permalink":"http://yoursite.com/tags/postgis/"}]},{"title":"Sqoop2 同步mysql 至 HDFS","date":"2016-12-08T02:14:53.000Z","path":"2016/12/08/Sqoop2-同步mysql-至-HDFS/","text":"1. 开启客户端sqoop2-shell 2. 配置sqoop server参数sqoop:000&gt; set server --host luhuijundeMacBook-Pro.local --port 12000 --webapp sqoop #luhuijundeMacBook-Pro.local 一般为HDFS主机名 –webapp官方文档说是指定的sqoop jetty服务器名称， 大概是一个自己能识别的用于标示这个服务器的名字吧。 3. 我们在使用的过程中可能会遇到错误，使用此配置打印错误sqoop:000&gt; set option --name verbose --value true 4. 验证是否已经连上sqoop:000&gt; show version --all #如果server version:能显示代表能正常连接 5. 使用help命令可以查看sqoop支持的所有命令1234567891011121314151617sqoop:000&gt; helpFor information about Sqoop, visit: http://sqoop.apache.org/Available commands: exit (\\x ) Exit the shell history (\\H ) Display, manage and recall edit-line history help (\\h ) Display this help message set (\\st ) Configure various client options and settings show (\\sh ) Display various objects and configuration options create (\\cr ) Create new object in Sqoop repository delete (\\d ) Delete existing object in Sqoop repository update (\\up ) Update objects in Sqoop repository clone (\\cl ) Create new object based on existing one start (\\sta) Start job stop (\\stp) Stop job status (\\stu) Display status of a job enable (\\en ) Enable object in Sqoop repository disable (\\di ) Disable object in Sqoop repository 6. 检查Sqoop server支持的连接sqoop:000&gt; show connector 7. 创建数据源头link12345678910111213141516sqoop:000&gt; create link -connector generic-jdbc-connectorName: First LinkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://mysql.server/databaseNameUsername:dbaPassword: *****Fetch Size:(回车)entry#protocol=tcpentry#(回车)Identifier enclose:(空格) #这里是指定SQL中标识符的定界符，也就是说，有的SQL标示符是一个引号：select * from &quot;table_name&quot;，这种定界符在MySQL中是会报错的。这个属性默认值就是双引号，所以不能使用回车，必须将之覆盖，我使用空格覆盖了这个值。官方文档这里有坑！New link was successfully created with validation status OK and name third link 看到这样的字符这个link算是创建成功 8. 创建目标link1234567891011sqoop:000&gt; create link -connector hdfs-connectorCreating link for connector with name hdfs-connectorPlease fill following values to create new link objectName: Second LinkLink configurationHDFS URI: hdfs://localhost:9000Conf directory:/usr/local/hadoop/etc/hadoopentry#(回车)New link was successfully created with validation status OK and name Second Link 看到这样的字符串代表创建成功 9. 使用两个link名字 from 和 to 来创建job12345678910111213141516171819202122232425262728293031323334353637383940414243444546sqoop:000&gt; create job -f &quot;First Link&quot; -t &quot;Second Link&quot; Creating job for links with from name First Link and to name Second Link Please fill following values to create new job object Name: Sqoopy #job 名称Schema name: test #mysql数据库名称Table name: test #表名称SQL statement:Column names:There are currently 0 values in the list:element#(回车)Partition column:(回车)Partition column nullable:(回车)Boundary query:(回车)Incremental readCheck column:(回车)Last value:(回车)Target configuration #配置目标Override null value: nullNull value: nullFile format: 0 : TEXT_FILE 1 : SEQUENCE_FILE 2 : PARQUET_FILEChoose: 0 #选择0最简单的文本文件Compression codec: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0 #选择0,不压缩Custom codec:(回车)Output directory: hdfs://localhost:9000/user/luhuijun/sqoop #最好是完全没有这个目录: sqoop,如果有目录里面又有文件, 又是一堆权限问题.Append mode:(回车)Throttling resourcesExtractors: 2Loaders: 2Classpath configurationExtra mapper jars:There are currently 0 values in the list:element#New job was successfully created with validation status OK and name Sqoopy 9. 开启job 并打印job执行详情12345678910111213141516sqoop:000&gt; start job -n Sqoopy -sSubmission detailsJob Name: SqoopyServer URL: http://luhuijundeMacBook-Pro.local:12000/sqoop/Created by: luhuijunCreation date: 2016-12-06 21:26:24 CSTLastly updated by: luhuijunExternal ID: job_1481030429951_0001 http://luhuijundeMacBook-Pro.local:8088/proxy/application_1481030429951_0001/Source Connector schema: Schema&#123;name= test . test ,columns=[ FixedPoint&#123;name=id,nullable=true,type=FIXED_POINT,byteSize=4,signed=true&#125;, Text&#123;name=name,nullable=true,type=TEXT,charSize=null&#125;]&#125;2016-12-06 21:26:24 CST: BOOTING - Progress is not available2016-12-06 21:26:36 CST: RUNNING - 0.00 %2016-12-06 21:26:46 CST: RUNNING - 50.00 %2016-12-06 21:26:57 CST: SUCCEEDED 10. 查看执行结果1234567➜ /usr/local/hadoop/etc/hadoop git:(master) &gt;hdfs dfs -cat &apos;sqoop/*&apos; #这是zsh *直接匹配不到,在正常的shell里应该不需要引号1,&apos;1&apos;2,&apos;xinle&apos;3,&apos;huijun&apos;4,&apos;hongna&apos; 11. 总结看上去简单的几步,其实踩了很多坑,在学习过程中通常会犯两类错误：第一 类错误是在知之不多的情况下就盲目开始,即行动太快；第二类错误是在行动之前准备过多,即行动太晚。要想在这 二者之间取得平衡，你掌握的知识要恰到好处，足以能让你开始学习， 但又不会多到让你无力探索，这样学习效果最佳。&lt;&lt;软技能 代码之外的生存指南&gt;&gt;","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]},{"title":"Sqoop2 version 1.99.7 安装部署","date":"2016-12-08T02:13:14.000Z","path":"2016/12/08/Sqoop2-version-1-99-7-安装部署/","text":"1. 版本对比Sqoop2 相比Sqoop1 升级幅度太大,可以说两个软件完全没有关系.Sqoop2 相比Sqoop1 增加了server端, sqoop1 是那种解压出来配置个环境变量就能直接使用的软件, sqoop2 安装部署使用复杂,而且官方给出来的文档有几个坑要踩踩. 2. Sqoop2 Version 1.99.7 安装部署下载软件:wget http://apache.fayea.com/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz 解压软件:tar -zvxf sqoop-1.99.7-bin-hadoop200.tar.gz 存放到指定路径:mv sqoop-1.99.7-bin-hadoop200 /usr/local/sqoop 赋权限:chmod -R 755 /usr/local/sqoop 修改环境变量:vim /etc/profile 最下方添加如下变量: 123456export SQOOP_HOME=/usr/local/sqoopexport PATH=$SQOOP_HOME/bin:$PATHexport CATALINA_BASE=$SQOOP_HOME/serverexport LOGDIR=$SQOOP_HOME/logsexport SQOOP_SERVER_EXTRA_LIB=/usr/local/sqoop/server/lib指定SQOOP_SERVER_EXTRA_LIB 后续要把连接mysql的jar包 copy 至这个文件加下 copy mysql-connector jar包至指定文件: cp /Users/luhuijun/Downloads/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $SQOOP_HOME/server/lib 修改配置文件:sudo vim /usr/local/sqoop/conf/sqoop.properties 修改指向我的hadoop配置文件目录:org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/usr/local/hadoop/etc/hadoop 配置catalina.properties 此文件不存在,需要自已建立:vim /usr/local/sqoop/conf/catalina.properties 内容如下: 1common.loader=/usr/local/hadoop/share/hadoop/common/*.jar,/usr/local/hadoop/share/hadoop/common/lib/*.jar,/usr/local/hadoop/share/hadoop/hdfs/*.jar,/usr/local/hadoop/share/hadoop/hdfs/lib/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/lib/*.jar,/usr/local/hadoop/share/hadoop/tools/*.jar,/usr/local/hadoop/share/hadoop/tools/lib/*.jar,/usr/local/hadoop/share/hadoop/yarn/*.jar,/usr/local/hadoop/share/hadoop/yarn/lib/*.jar,/usr/local/hadoop/share/hadoop/httpfs/tomcat/lib/*.jar, 修改启动脚本: 12345678vim /usr/local/sqoop/bin/sqoop.sh添加如下内容,注意自己的路径:export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home HADOOP_COMMON_HOME=/usr/local/hadoop/share/hadoop/common HADOOP_HDFS_HOME=/usr/local/hadoop/share/hadoop/hdfs HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce HADOOP_YARN_HOME=/usr/local/hadoop/share/hadoop/yarn 修改hadoop的yarn-site.xml: 1234567vim /usr/local/hadoop/etc/hadoop/yarn-site.xml添加如下属性:&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 修改hadoop的container-executor.cfg: 1234vim /usr/local/hadoop/etc/hadoop/container-executor.cfg修改如些配置:allowed.system.users=luhuijun 配置为当前用户名, 不然sqoop2-shell去访问hdfs 文件的时候会报错(user: luhuijun is not allowed to impersonate luhuijun)这个问题坑了我好久 修改core-site.xml: 1234567891011vim /usr/local/hadoop/etc/hadoop/core-site.xml添加如下配置:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.groups&lt;/name&gt; &lt;value&gt;staff&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.hosts&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; 跟修改上一个配置是解决同一个问题,sqoop2-shell去访问hdfs的时候会报(user: luhuijun is not allowed to impersonate luhuijun),staff 为当前系统用户的用户组, 使用groups能查到当前用户组名. 启动sqoop 服务:sqoop.sh server start 3. 验证启动是否成功12345sqoop.sh client 或 sqoop2-shell 进入客户端set server --host hadoopMaster --port 12000 --webapp sqoop 设置服务器，注意hadoopMaster为hdfs主机名show connector --all 查看连接类型show link 查看连接show job 查看job","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]}]