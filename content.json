[{"title":"Mac上安装单机版Spark","date":"2016-12-13T08:18:32.000Z","path":"2016/12/13/Mac上安装单机版Spark/","text":"下载地址: http://spark.apache.org/downloads.html1. 点击下载地址,现在最新版本是spark-2.0.1-bin-hadoop2.7.tgz ,大概180MB.2. 解压下载文件,并将这个文件放大你想要的位置.3. 然后这就算安装好了..mv spark-2.0.1-bin-hadoop /usr/local/ 4. 测试1234567891011121314151617181920--进入安装目录cd /usr/local/spark--启动./sbin/start-master.sh--打开浏览器输入,查看主机运行情况http://localhost:8080--加子嗣(现在主机是司令,要给他派点兵.)./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT--这是后台启动nohup spark-class org.apache.spark.deploy.worker.Worker spark:/192.168.70.145:7077 &amp;--回到浏览器http://localhost:8080在“Workers”列表下赫然出现了你的第一个子嗣。其状态State为ALIVE。表示它正在运作。这时候你需要记住，当前Terminal的窗口对应就是那个刚加入的Worker Id。删除子嗣工作完成后吧小崽子们放出去玩,只需要在哪个看似死机的teminal 里按下CTRL+C关闭主机./sbin/stop-master.sh 5. WordCount脚本1234567891011进入spark-shell./bin/spark-shellimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.SparkContext._var conf=new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyApp&quot;)val sc=new SparkContext(conf)val input=sc.textFile(&quot;README.md&quot;) //这里可以设置为绝对路径val words=input.flatMap(line=&gt;line.split(&quot; &quot;))val counts=words.map(word=&gt;(word,1)).reduceByKey&#123;case (x,y)=&gt;x+y&#125;counts.saveAsTextFile(outputFile) //这里可以设置为绝对路径 6. 在Intellij IDEA里执行WordCount12345678910111213141516171819import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConfimport org.scalatest._object SimpleApp &#123; def main(args: Array[String]) &#123; var logFile=&quot;/usr/local/spark/README.md&quot; var conf=new SparkConf().setAppName(&quot;Simple Application&quot;).setMaster(&quot;local&quot;) var sc=new SparkContext(conf) val data=Array(1,2,3,4,5) val distData=sc.parallelize(data) val sumArray=distData.reduce((a,b)=&gt;a+b) val logData=sc.textFile(logFile,2).cache() val numAs=logData.filter(line=&gt;line.contains(&quot;a&quot;)).count() val numBs=logData.filter(line=&gt;line.contains(&quot;b&quot;)).count() println(&quot;Lines with a:%s,Lines with b:%s&quot;.format(numAs,numBs)) println(&quot;sumArray is:&quot;+sumArray) &#125;&#125;","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"IntelliJ IDEA 消除Spark程序日志中INFO输出","date":"2016-12-12T16:01:05.000Z","path":"2016/12/13/IntelliJ-IDEA-消除Spark程序日志中INFO输出/","text":"说明: 在使用Intellij IDEA，local模式下运行Spark程序时，会在Run窗口打印出很多INFO信息，辅助信息太多可能会将有用的信息掩盖掉。如下所示这里写图片描述123456789101112131415161718192021222324252627282930/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/bin/java -Didea.launcher.port=7532 &quot;-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin&quot; -Dfile.encoding=UTF-8 -classpath &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/lib/tools.jar:/Users/luhuijun/mydoc/spark/target/classes:/Users/luhuijun/.m2/repository/org/scala-lang/scala-library/2.11.0/scala-library-2.11.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-core_2.11/2.0.1/spark-core_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/luhuijun/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/luhuijun/.m2/repository/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/Users/luhuijun/.m2/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/Users/luhuijun/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/Users/luhuijun/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/Users/luhuijun/.m2/repository/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/Users/luhuijun/.m2/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-common/2.2.0/hadoop-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/luhuijun/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/luhuijun/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/luhuijun/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/luhuijun/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/luhuijun/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/luhuijun/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/luhuijun/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-auth/2.2.0/hadoop-auth-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.2.0/hadoop-hdfs-2.2.0.jar:/Users/luhuijun/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.2.0/hadoop-mapreduce-client-app-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.2.0/hadoop-mapreduce-client-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.2.0/hadoop-yarn-client-2.2.0.jar:/Users/luhuijun/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/Users/luhuijun/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/luhuijun/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.2.0/hadoop-yarn-server-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.2.0/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.2.0/hadoop-yarn-api-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.2.0/hadoop-mapreduce-client-core-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.2.0/hadoop-yarn-common-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.2.0/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/hadoop/hadoop-annotations/2.2.0/hadoop-annotations-2.2.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-launcher_2.11/2.0.1/spark-launcher_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-network-common_2.11/2.0.1/spark-network-common_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.5/jackson-annotations-2.6.5.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-network-shuffle_2.11/2.0.1/spark-network-shuffle_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-unsafe_2.11/2.0.1/spark-unsafe_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/luhuijun/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/luhuijun/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/luhuijun/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/luhuijun/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/luhuijun/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/Users/luhuijun/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/Users/luhuijun/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/Users/luhuijun/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/luhuijun/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/Users/luhuijun/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/luhuijun/.m2/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/Users/luhuijun/.m2/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Users/luhuijun/.m2/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/luhuijun/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/Users/luhuijun/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/Users/luhuijun/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/Users/luhuijun/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/Users/luhuijun/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/Users/luhuijun/.m2/repository/org/apache/mesos/mesos/0.21.1/mesos-0.21.1-shaded-protobuf.jar:/Users/luhuijun/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar:/Users/luhuijun/.m2/repository/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar:/Users/luhuijun/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/luhuijun/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/Users/luhuijun/.m2/repository/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/Users/luhuijun/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/Users/luhuijun/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/luhuijun/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/luhuijun/.m2/repository/net/razorvine/pyrolite/4.9/pyrolite-4.9.jar:/Users/luhuijun/.m2/repository/net/sf/py4j/py4j/0.10.3/py4j-0.10.3.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-tags_2.11/2.0.1/spark-tags_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar:/Users/luhuijun/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/Users/luhuijun/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-sql_2.11/2.0.1/spark-sql_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/com/univocity/univocity-parsers/2.1.1/univocity-parsers-2.1.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-sketch_2.11/2.0.1/spark-sketch_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-catalyst_2.11/2.0.1/spark-catalyst_2.11-2.0.1.jar:/Users/luhuijun/.m2/repository/org/codehaus/janino/janino/2.7.8/janino-2.7.8.jar:/Users/luhuijun/.m2/repository/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-column/1.7.0/parquet-column-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-common/1.7.0/parquet-common-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-encoding/1.7.0/parquet-encoding-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-generator/1.7.0/parquet-generator-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-hadoop/1.7.0/parquet-hadoop-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-format/2.3.0-incubating/parquet-format-2.3.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/parquet/parquet-jackson/1.7.0/parquet-jackson-1.7.0.jar:/Users/luhuijun/.m2/repository/org/apache/spark/spark-hive_2.11/2.0.2/spark-hive_2.11-2.0.2.jar:/Users/luhuijun/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/Users/luhuijun/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark2/hive-exec-1.2.1.spark2.jar:/Users/luhuijun/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/luhuijun/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/Users/luhuijun/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/luhuijun/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/Users/luhuijun/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/luhuijun/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/luhuijun/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/luhuijun/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/luhuijun/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/luhuijun/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/luhuijun/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/Users/luhuijun/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/Users/luhuijun/.m2/repository/org/json/json/20090211/json-20090211.jar:/Users/luhuijun/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/luhuijun/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/luhuijun/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark2/hive-metastore-1.2.1.spark2.jar:/Users/luhuijun/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/luhuijun/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/luhuijun/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/luhuijun/.m2/repository/org/apache/derby/derby/10.10.2.0/derby-10.10.2.0.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/luhuijun/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/luhuijun/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/luhuijun/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/luhuijun/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/luhuijun/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/luhuijun/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/luhuijun/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/Users/luhuijun/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/Users/luhuijun/.m2/repository/org/codehaus/janino/commons-compiler/2.7.6/commons-compiler-2.7.6.jar:/Users/luhuijun/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/Users/luhuijun/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/Users/luhuijun/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/luhuijun/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/Users/luhuijun/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/Users/luhuijun/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/luhuijun/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/luhuijun/.m2/repository/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar:/Users/luhuijun/.m2/repository/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar&quot; com.intellij.rt.execution.application.AppMain SparkSql16/12/12 23:46:59 INFO SparkContext: Running Spark version 2.0.116/12/12 23:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable16/12/12 23:47:04 INFO SecurityManager: Changing view acls to: luhuijun16/12/12 23:47:04 INFO SecurityManager: Changing modify acls to: luhuijun16/12/12 23:47:04 INFO SecurityManager: Changing view acls groups to: 16/12/12 23:47:04 INFO SecurityManager: Changing modify acls groups to: 16/12/12 23:47:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(luhuijun); groups with view permissions: Set(); users with modify permissions: Set(luhuijun); groups with modify permissions: Set()16/12/12 23:47:04 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 55294.16/12/12 23:47:04 INFO SparkEnv: Registering MapOutputTracker16/12/12 23:47:04 INFO SparkEnv: Registering BlockManagerMaster16/12/12 23:47:04 INFO DiskBlockManager: Created local directory at /private/var/folders/nm/3bv6gdms53115l22n2vhsypc0000gn/T/blockmgr-58f045b4-0e51-43fa-80dd-0af1c2482dc416/12/12 23:47:04 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB16/12/12 23:47:04 INFO SparkEnv: Registering OutputCommitCoordinator16/12/12 23:47:04 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.16/12/12 23:47:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.3.2:404016/12/12 23:47:04 INFO Executor: Starting executor ID driver on host localhost16/12/12 23:47:04 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 55295.16/12/12 23:47:08 INFO DAGScheduler: ResultStage 6 (show at SparkSql.scala:24) finished in 0.016 s16/12/12 23:47:08 INFO DAGScheduler: Job 4 finished: show at SparkSql.scala:24, took 0.019567 s16/12/12 23:47:08 INFO CodeGenerator: Code generated in 6.561389 ms+-------+| name|+-------+|Michael|| Andy|| Justin|+-------+结果隐藏在其中根本找不到... 解决方案: 要解决这个问题，主要是要正确设置好log4j文件，本文主要分析如何在local模式下，将Spark的INFO信息隐藏，不影响程序中的结果输出。1、在项目src路径下创建resources文件夹, 右击该文件Mark Directory as 选中Resources Root2、将spark根目录下的log4j.properties文件复制 到 src/resources文件夹下 3、修改log4j.properties文件的内容 将第一行的log4j.rootCategory=INFO, console改成log4j.rootCategory=ERROR, console，只显示ERROR级别的日志。测试: 再次运行该代码，可以看到INFO信息已经消失这里写图片描述123456789101112131415161718192021/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/bin/java -Didea.launcher.port=7533 &quot;-Didea.launcher.bin.path=/Applications/IntelliJ IDEA.app/Contents/bin&quot; -Dfile.encoding=UTF-8 -classpath &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/+------------+| value|+------------+|Name: Justin|+------------++------------+| value|+------------+|Name: Justin|+------------++---------------+----+| address|name|+---------------+----+|[Columbus,Ohio]| Yin|+---------------+----+Process finished with exit code 0","tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"浅谈事务和锁","date":"2016-12-12T15:14:13.000Z","path":"2016/12/12/浅谈事务和锁/","text":"本篇文章使用SQL Server来讲解原因: 是因为SQL Server默认支持悲观并发,并且乐观并发的种类比较多,Mysql和Postgresql直接默认就是乐观并发. 感觉用SQL Server来讲会更加全面, 但是不好的是我早就没有SQL Server的IDE了,因为好几年没有接触过SQL Server数据库了. 1. 原子性: 最小单位不可分割,要么全部提交,要么全部回滚.转账的例子:1234567begin transaction;-- 从我账户扣钱update balance=balance-500 where id=&apos;my_id&apos;;-- 向目标账户加钱update balance=balance+500 where id=&apos;target_id&apos;;commit;不存在扣了我的钱,目标账户没有加钱的情况. 2. 一致性: 实现动态平衡,扣了我500元,加了他500元,但是我的总额不变.3. 隔离性: 事务之间相互隔离,不受影响。锁就跟此特性息息相关，不同隔离级别共享锁持有的时间是不一样的。3.1 排他锁持有时间就不说了，为了保证事务的一致性，无论什么隔离级别排他锁都是从事务的开始一直持续到事务结束。如果此语句没有加begin transcation就算一个隐式事务，也就是从语句开始执行持续到语句执行结束。3.2 现在了解一下不同隔离级别对共享锁的影响。3.2.1 读未提交: 从语意上来讲，别人没有提交你能读，那么你读取的时候肯定是不加共享锁的，如果加了共享锁就违背了互斥原理 S排斥X，但是此时我们读到的数据为脏数据（既没有提交，也没有回滚的数据称为脏数据，是一个中间状态）。12345-- 两种方法能读取到脏数据select * from TableName where id=1000 with(nolock);SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;select * from TableName where id=1000 3.2.2 读已提交,也是SQL Server默认的隔离级别,所以SQL Server默认隔离级别为悲观并发，从语意上来讲，别人提交了你能读，但是共享锁是读取一条释放一条，读取一条释放一条，而不是持续到事务的结束。12345678begin transaction;-- 读完就释放select * from TableName where id=1000;-- 读完就释放select * from TableName where id=1000;-- 所以第一次读取和第二次读取到的数据很有可能是不同的,如果第一次读完后，有其他事务修改了此行数据，那么第二次读取到的数据就会不一样。commit； 3.2.3 可重复读,从语意上来讲,可以重复读取,也就是在同一个事务内不管读取多少次,读取到的值都是一样。那么这个隔离级别的共享锁一定是持续到事务结束的，这段时间内其他事务不能修改。123456789begin transaction;-- 读完不释放select * from TableName where id=1000；-- 等待20秒waitfor delay &apos;00:00:20&apos; -- 读完不释放select * from TableName where id=1000;-- 直到执行commit 的时候共享锁才会释放, 所以每次读取到的数据是一样的.commit; 3.2.4 可序列化,从语意上来讲,可以序列执行,他不但能持续到事务结束,还会在聚集键上上一个范围锁,range锁,这段时间内,不但不能修改这个范围的数据,就连插入都不可以。如果此表没有聚集索引，没有聚集键，那么这个共享锁是加在表上的，原因：SQL Server有堆表的概念，存放的直接是数据的物理位置（FileID：PageID：SlotNum）物理的东西没有范围的概念，磁盘上这一点数据，那一天数据很难给你上一个范围锁，所以表锁。1234-- 读完不释放,并且是key range锁,如果没有聚集索引是表锁select * from TableName where id&gt;1000 and id&lt;1005；-- 读完不释放并且是key range锁,如果没有聚集索引是表锁select * from TableName where id&gt;1000 and id&lt;1005； 3.2.5 读已提交(快照) 实现语句级别的读取一致性,读取的是行的老版本.3.2.6 快照, 实现事务级别的读取一致性,读取的是行的老版本. 但是此隔离级别会照成更新冲突。冲突发生是因为事务2在Quantity值为324的时候开始，当这个值被事务1更新后，行版本324被存储到版本存储区内。事务2会在事务的持续时间内继续读取该行数据。如果两个更新操作都被允许成功执行的话，就会产生经典的更新丢失情形。事务1增加了200个数量，然后事务2会在初始值上增加300个数量并存储。由第一个事务增加的那200个产品就会彻底丢失，SQL Server不会允许这样的情况发生。 4.持久性，事务一旦提交成功持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。即使出现了任何事故比如断电等，事务一旦提交，则持久化保存在数据库中.","tags":[{"name":"数据库原理","slug":"数据库原理","permalink":"http://yoursite.com/tags/数据库原理/"}]},{"title":"表设计短类型优势","date":"2016-12-12T09:03:43.000Z","path":"2016/12/12/表设计短类型优势/","text":"一、背景咱们业务数据库的表字段长度偏长，特别是varchar类型，全部都是varchar（255）；表现的比较慷慨，事实证明短类型有很大的优势。 二、详情描述使用varchar（5）和varchar（200）存放hello 有什么区别？变长类型对比使用varchar（5）和varchar（200）存放hello空间开销是一样的，但是短类型会有更大的优势；原因如下：更长的类型会消耗更多的内存，因为mysql会分配固定大小内存块来保存在内存中。更长的类型使用临时表进行排序的时候会特别糟糕，在利用磁盘临时表进行排序的时候同样糟糕。 如果数据库系统哪哪都有问题，先检查是不是内存问题？内存瓶颈会造成IO和CPU波动；内存应该算是数据库中最宝贵的资源，我们所有的架构设计，高性能方案；sharding分布式多点写入都是围绕着怎么才能不跟磁盘交互来完成业务逻辑实现；因为内存的速度和硬盘的速度不在同一个数量级，内存有压力的时候会把缓存的数据页清除；后续需要用到这部分数据时就会产生物理IO，速度大打折扣，同样内存有压力会清除执行计划。相同语句后续执行就需要重新编译，CPU压力也会有所增加。所以阿里云RDS租金都是跟内存成正比。更小的通常更快，因为他占用更小的磁盘，内存和CPU缓存，并且处理cpu周期也会更短，所以慷慨不是明智的选择。只分配真正需要的空间。 三、例子咱们系统里日期类型全都是datetime占用了8个字节，timestamp 不但有datetime的所有功能，一样的表现形式却只占用4个字节。通常这些列都是有索引的，那么timestamp类型的这个索引页存放的数量应该是datetime类型列的2倍，那么单次IO取的数据量应该是datetime的2倍，缓存命中率也将直线上升。","tags":[{"name":"Performance","slug":"Performance","permalink":"http://yoursite.com/tags/Performance/"}]},{"title":"解决爬虫数据(电影院比价系统)电影院名称不规范问题解决思路","date":"2016-12-12T08:20:38.000Z","path":"2016/12/12/解决爬虫数据-电影院比价系统-电影院名称不规范问题解决思路/","text":"1 问题各大网站录入电影院，地址没有统一的规范，造成电影票无法比价。 2 解决思路2.1 经纬度范围查找拿到数据中包含经度维度信息，根据经纬度范围查找锁定这些名字不同的电影院为同一家电影院。 2.1.1 各大网站使用的地图坐标协议不同（google、高德、腾讯、图吧地图、图吧导航）使用的是gcj02，百度、搜狗使用的是另外一种坐标协议bd09。所以网上找个java写的统一转换各大地图协议至百度地图的代码，然后改写为mysql的自定义函数，转换后误差在万分之五（距离大概是5-5.5米） 一、经纬度距离换算a）在纬度相等的情况下： 经度每隔0.00001度，距离相差约1米； 每隔0.0001度，距离相差约10米； 每隔0.001度，距离相差约100米； 每隔0.01度，距离相差约1000米； 每隔0.1度，距离相差约10000米。 b）在经度相等的情况下： 纬度每隔0.00001度，距离相差约1.1米； 每隔0.0001度，距离相差约11米； 每隔0.001度，距离相差约111米； 每隔0.01度，距离相差约1113米； 每隔0.1度，距离相差约11132米。 高德 convert to 百度经纬度函数（网上java有现成代码，这是根据java改写mysql代码）。各个地图经纬度转换 转换维度12345678910111213141516171819DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lat(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x*x+y*y)+ 0.00002 * sin(y*x_pi); SET theta=atan2(y,x)+ 0.000003 * cos(x*x_pi); SET longitude=z*cos(theta)+0.0065; SET latitude=z*sin(theta)+0.006; RETURN latitude;END |DELIMITER ; 测试SELECT convert_gcj02_to_bd09_lat(120.098703,29.324483); 转换经度123456789101112131415161718DELIMITER |CREATE FUNCTION convert_gcj02_to_bd09_lng(longitude DOUBLE(9,6),latitude DOUBLE(9,6))RETURNS DOUBLE(9,6)BEGIN DECLARE x_pi DOUBLE(9,8); DECLARE x DOUBLE(9,6); DECLARE y DOUBLE(9,6); DECLARE z DOUBLE(9,6); DECLARE theta DOUBLE(10,9); SET x_pi = 3.14159265358979324 * 3000.0 / 180.0; SET x=longitude; SET y=latitude; SET z=sqrt(x * x + y * y) + 0.00002 * sin(y * x_pi); SET theta = atan2(y, x) + 0.000003 * cos(x * x_pi); SET longitude = z * cos(theta) + 0.0065; RETURN longitude;END |DELIMITER ; 测试SELECT convert_gcj02_to_bd09_lng(120.098703,29.324483); 根据经纬度计算距离函数1234567DELIMITER |CREATE FUNCTION `juli`(lat1 DOUBLE(10,7),lat2 DOUBLE(10,7),lng1 DOUBLE(10,7),lng2 DOUBLE(10,7)) RETURNS doubleBEGINSET @distance=round(6378.138*2*asin(sqrt(pow(sin( (lat1*pi()/180-lat2*pi()/180)/2),2)+cos(lat1*pi()/180)*cos(lat2*pi()/180)* pow(sin( (lng1*pi()/180-lng2*pi()/180)/2),2)))*1000);RETURN @distance;END |DELIMITER ; 2.1.2 弃用经纬度算法很多影院的经纬度信息为null，而且有些经纬度信息不太准确，所以后面弃用了根据经纬度去定位是否为同一家影院。 3. 根据电影院名字，电话，地址的相识度匹配。3.1 公式如下count（相识单词之间A和B）/count(A)+count(B)-count(交集)) 3.2 代码如下：电影院名称相识度匹配 对比两个字符串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647DELIMITER ;;CREATE FUNCTION `levenshtein`( s1 TEXT, s2 TEXT) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, i, j, c, c_temp, cost INT; DECLARE s1_char CHAR; DECLARE cv0, cv1 TEXT; SET s1_len = CHAR_LENGTH(s1), s2_len = CHAR_LENGTH(s2), cv1 = 0x00, j = 1, i = 1, c = 0; IF s1 = s2 THEN RETURN 0; ELSEIF s1_len = 0 THEN RETURN s2_len; ELSEIF s2_len = 0 THEN RETURN s1_len; ELSE WHILE j &lt;= s2_len DO SET cv1 = CONCAT(cv1, UNHEX(HEX(j))); SET j = j + 1; END WHILE; WHILE i &lt;= s1_len DO SET s1_char = SUBSTRING(s1, i, 1); SET c = i; SET cv0 = UNHEX(HEX(i)); SET j = 1; WHILE j &lt;= s2_len DO SET c = c + 1; IF s1_char = SUBSTRING(s2, j, 1) THEN SET cost = 0; ELSE SET cost = 1; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j, 1)), 16, 10) + cost; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET c_temp = CONV(HEX(SUBSTRING(cv1, j+1, 1)), 16, 10) + 1; IF c &gt; c_temp THEN SET c = c_temp; END IF; SET cv0 = CONCAT(cv0, UNHEX(HEX(c))); SET j = j + 1; END WHILE; SET cv1 = cv0; SET i = i + 1; END WHILE; END IF; RETURN c; END ;;DELIMITER ; 两个字符串相识度占比1234567891011121314DELIMITER ;;CREATE FUNCTION `levenshtein_ratio`( s1 TEXT, s2 TEXT ) RETURNS INT(11) DETERMINISTICBEGIN DECLARE s1_len, s2_len, max_len INT; SET s1_len = LENGTH(s1), s2_len = LENGTH(s2); IF s1_len &gt; s2_len THEN SET max_len = s1_len; ELSE SET max_len = s2_len; END IF; RETURN ROUND((1 - LEVENSHTEIN(s1, s2) / max_len) * 100); END |DELIMITER ;; 通过几次测试，相识度大于等于90的大致为同一影院。个别电影院名字极度相仿的，可以对相识度值做一些调整。12SELECT *,levenshtein_ratio(&apos;龙海金逸影城(美一店)&apos;,cinema_name) xiangshi FROM `bidding_cinema_data`WHERE levenshtein_ratio(&apos;龙海金逸影城(美一店)&apos;,cinema_name)&gt;=90; 4. 重回经纬度字符串匹配的精确度很难达到80以上(因为有的电影院名字很短,只有两个字或4个字)所以这些电影院相识度匹配的时候,很难区分… 4.1 问题采集到的数据,有的经纬度信息为null所以根据百度地图接口传入地址来补全经纬度信息. 4.2 根据经纬度范围打标签SQL脚本如下:打标签第一版本 123456789101112131415161718192021222324252627282930DELIMITER ;;CREATE PROCEDURE `set_lable`(lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id int,lables int)-- lng:维度-- lat:经度-- rounds:前后范围-- city_meta_id:城市编号-- labels:标签BEGIN set @lng=lng; set @lat=lat; set @rounds=rounds; set @lable=lables; set @city_meta_id=city_meta_id; update clean_cinema_data_copy as a inner join bidding_city_data as b on a.city_id=b.city_id and a.site_id=b.site_id SET lable=@lable, brand=replace(replace(replace(replace(replace(replace(replace(replace(cinema_name,&apos;电影院&apos;,&apos; &apos;),&apos;电影城&apos;,&apos; &apos;),&apos;影视城&apos;,&apos; &apos;),&apos;国际&apos;,&apos;&apos;),&apos;影院&apos;,&apos; &apos;),&apos;影城&apos;,&apos; &apos;),&apos;影视&apos;,&apos; &apos;),city_name,&apos; &apos;) where longitude&lt;&gt;0.0 and city_meta_id=@city_meta_id and latitude&gt;= @lat-@rounds and latitude&lt;@lat+@rounds and longitude&gt;= @lng-@rounds and longitude&lt;@lng+@rounds and lable is NULL; END;;DELIMITER ; 4.3 调用上面过程的脚本批量打标签第一版本 1234567set @rownum=0;selectconcat(&apos;call set_lable(&apos;,longitude,&apos;,&apos;,latitude,&apos;,&apos;,0.006,&apos;,&apos;,3120,&apos;,&apos;,@rownum:=@rownum+1,&apos;);&apos;)from clean_cinema_data_copywhere city_meta_id=3120and longitude&lt;&gt;0.0;-- 把此语句执行的结果复制到连接数据库的IDE里执行 4.4 根据经纬度范围打标签结果12345678910111213141516最终的准确度在65%-75% 之间, 距离最终90%还有一定距离. 所以后面会加上一些brand的词库. 根据经纬度范围打过标签之后再根据brand这个维度再打一次.上海市千分之六:大于等于4家的是128个, 等于4家的是91个;千分之三:大于等于4家的是139个, 等于4家的是113个;千分之二:大于等于4家的是142个, 等于4家的是125个; SELECT 125*1.0/165 准确度:0.75758; 北京市千分之六:大于等于4家的是110个, 等于4家的是83个; select 83*1.0/136 0.61029;千分之三:大于等于4家的是107个, 等于4家的是92个; select 92*1.0/136 准确度: 0.67647;千分之二:大于等于4家的是99个, 等于4家的是89个; SELECT 89*1.0/136 0.65441; 广州市千分之六:大于等于4家的是78个, 等于4家的是58个; select 58*1.0/105 0.55238;千分之三:大于等于4家的是79个, 等于4家的是68个; select 68*1.0/105 准确度:0.64762;千分之二:大于等于4家的是76个, 等于4家的是66个; SELECT 66*1.0/105 0.62857; 5. 根据经纬度范围和词库brand两个维度打标签的准确率思路根据两个经纬度打标签,打完标签, 本来5个的6个的7个的可能会分出来1,2,3条,再加上一个维度打标签,完全为4个电影院的准确率为 上海市两个维度打标签 0.7879 上海由原来的0.75758 提升为0.75758 北京市两个维度打标签 0.7353 北京由原来的0.67647提升为0.7353 广州市两个维度打标签 0.6762 广州由原来的0.64762提升为0.6762准确率还是不太高…… 6. 加维度逻辑如下: 6.1 首先根据经纬度的范围打一次标签(把范围在200 米内的并且有4家[4个网站] 算为一个电影院) {结果集1}6.2 再把范围200米内不为4家(有的比较密集,8家,12家)加上简单的品牌分词, 按 机器标签, 品牌分组 等于4个的集合 {结果集2}6.3 再把城市所有数据 跟上面两个结果集的数据的交集求并集{结果集3}6.4 贪心算法 应用到结果集合3, 从500米开始步长循环处理每次递减50米…把完全等于4的集合放入临时表…(这里会产生9个临时表)6.5 创建过程把 集合1 U distinct {临时表1 U 临时表2 U 临时表 U 临时表3 U 临时表4 U 临时表5 U 临时表6 U 临时表7 U 临时表8 U 临时表9}取出来就是该城市最终数据, 上面公式中文解释 9个临时表取并集 去除重复后 跟集合1 取交集… 7. 打标签代码如下:打标签第二版本 12345678910111213141516171819202122232425262728293031323334DELIMITER ;;CREATE PROCEDURE `set_lable1`(tablename VARCHAR(48),lng DOUBLE,lat DOUBLE,rounds DOUBLE,city_meta_id INT)BEGIN DECLARE a INT DEFAULT 1; SET @tablename=tablename; SET @lng=lng; SET @lat=lat; SET @rounds=rounds; SET @city_meta_id=city_meta_id; SET @v_sql=CONCAT(&apos;SELECT ifnull(max(lable)+1,1) INTO @nums FROM &apos;,@tablename); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt; SET @v_sql=CONCAT(&apos;UPDATE &apos;,@tablename,&apos; AS a INNER JOIN bidding_city_data AS b ON a.city_id=b.city_id AND a.site_id=b.site_id SET lable=&apos;,@nums,&apos;,&apos;, &apos; brand=LEFT(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(cinema_name,&quot;电影院&quot;,&quot;&quot;),&quot;电影城&quot;,&quot;&quot;),&quot;影视城&quot;,&quot;&quot;),&quot;国际&quot;,&quot;&quot;),&quot;影院&quot;,&quot;&quot;),&quot;影城&quot;,&quot;&quot;),&quot;影视&quot;,&quot;&quot;),city_name,&quot;&quot;),&quot;（&quot;,&quot;&quot;),&quot;(&quot;,&quot;&quot;),&quot;）&quot;,&quot;&quot;),&quot;)&quot;,&quot;&quot;),&quot;影剧院&quot;,&quot;&quot;),&quot;-&quot;,&quot;&quot;),&quot; &quot;,&quot;&quot;),3) WHERE longitude&lt;&gt;0.0 AND city_meta_id=&apos;,@city_meta_id,&apos; AND latitude&gt;=&apos;, @lat-@rounds, &apos; AND latitude&lt; &apos;,@lat+@rounds, &apos; AND longitude&gt;=&apos;, @lng-@rounds,&apos; AND longitude&lt; &apos;,@lng+@rounds, &apos; AND lable IS NULL;&apos;); PREPARE stmt FROM @v_sql; EXECUTE stmt; DEALLOCATE PREPARE stmt;END;;DELIMITER ; 8. 批量打标签脚本批量打标签第二版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299delimiter |CREATE PROCEDURE batch_set_lable1(city INT,rounds DOUBLE,groups INT)BEGIN DECLARE done INT DEFAULT -1; DECLARE lng DOUBLE; DECLARE lat DOUBLE; DECLARE cur CURSOR FOR SELECT longitude,latitude FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0.0; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done=1; OPEN cur; read_loop:LOOP FETCH cur INTO lng,lat; IF done=1 THEN LEAVE read_loop; END IF; CALL set_lable1(&apos;clean_cinema_data_copy&apos;,lng,lat,rounds,city); END LOOP; CLOSE cur; -- 根据经纬度范围0.002打标签和 -- 每组不等于4个的数据再根据品牌分组等于4的集合, -- 此集合与上海市的全部数据的差集, -- 是我们后续需要缩小经纬度分析的集合 DROP TABLE IF EXISTS step_5_0; CREATE TABLE step_5_0 AS SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy AS a INNER JOIN ( SELECT * FROM clean_cinema_data_copy WHERE city_meta_id=city GROUP BY `lable` HAVING count(1)=groups ) AS b ON a.`lable`=b.`lable` AND a.city_meta_id=city UNION SELECT c.* FROM clean_cinema_data_copy AS c INNER JOIN ( SELECT * FROM ( SELECT a.* FROM clean_cinema_data_copy a INNER JOIN ( SELECT lable, id, longitude, latitude, cinema_name, cinema_meta_id, brand, COUNT(DISTINCT cinema_meta_id) cinemas, COUNT(1) counts FROM clean_cinema_data_copy WHERE city_meta_id=city AND longitude&lt;&gt;0 GROUP BY lable HAVING count(1)&lt;&gt;groups )b ON a.lable=b.lable AND a.brand=b.`brand` GROUP BY lable,brand HAVING count(1)=groups )tb GROUP BY lable,brand )d ON c.lable=d.`lable` AND c.brand=d.brand AND c.city_meta_id )h; -- 处理500米内的数据 SET @rownum=0; DROP TABLE IF EXISTS tmp_5_0; CREATE TABLE tmp_5_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_5_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_5_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_5_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_5_0&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理450米内的数据 DROP TABLE IF EXISTS tmp_4_5; CREATE TABLE tmp_4_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_5&apos;,lng,lat,0.0045,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理400米内的数据 DROP TABLE IF EXISTS tmp_4_0; CREATE TABLE tmp_4_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_4_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_4_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_4_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_4_0&apos;,lng,lat,0.004,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理350米内的数据 DROP TABLE IF EXISTS tmp_3_5; CREATE TABLE tmp_3_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_5&apos;,lng,lat,0.0035,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理300米内的数据 DROP TABLE IF EXISTS tmp_3_0; CREATE TABLE tmp_3_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_3_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_3_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_3_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_3_0&apos;,lng,lat,0.003,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理250米内的数据 DROP TABLE IF EXISTS tmp_2_5; CREATE TABLE tmp_2_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_5&apos;,lng,lat,0.005,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理200米内的数据 DROP TABLE IF EXISTS tmp_2_0; CREATE TABLE tmp_2_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_2_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_2_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_2_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_2_0&apos;,lng,lat,0.002,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理150米内的数据 DROP TABLE IF EXISTS tmp_1_5; CREATE TABLE tmp_1_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_5&apos;,lng,lat,0.0015,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理100米内的数据 DROP TABLE IF EXISTS tmp_1_0; CREATE TABLE tmp_1_0 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_1_0 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_1_0; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_1_0 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_1_0&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; -- 处理50米内的数据 DROP TABLE IF EXISTS tmp_0_5; CREATE TABLE tmp_0_5 SELECT *,@rownum:=@rownum+1 orders FROM clean_cinema_data_copy WHERE `city_meta_id`=city AND (longitude&lt;&gt;0.0 OR latitude&lt;&gt;0.0) AND id NOT IN ( SELECT id FROM step_5_0 ); UPDATE tmp_0_5 SET lable=NULL; SET @loopstart=1; SELECT @loopend:=max(orders) FROM tmp_0_5; WHILE @loopstart&lt;=@loopend DO BEGIN SELECT longitude,latitude INTO lng,lat FROM tmp_0_5 WHERE orders=@loopstart; CALL set_lable1(&apos;tmp_0_5&apos;,lng,lat,0.001,city); SET @loopstart=@loopstart+1; END; END WHILE; END |DELIMITER ; 9. 获取最终结果的过程获取结果的过程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697delimiter |CREATE PROCEDURE cinema_result(groups INT)begin SELECT id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name`,concat(&apos;step_&apos;,lable) lable FROM step_5_0 UNION SELECT * FROM ( SELECT DISTINCT id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name`,lable FROM ( SELECT * FROM tmp_5_0 WHERE lable IN ( SELECT lable FROM tmp_5_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_5 WHERE lable IN ( SELECT lable FROM tmp_4_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_4_0 WHERE lable IN ( SELECT lable FROM tmp_4_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_5 WHERE lable IN ( SELECT lable FROM tmp_3_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_3_0 WHERE lable IN ( SELECT lable FROM tmp_3_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_5 WHERE lable IN ( SELECT lable FROM tmp_2_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_2_0 WHERE lable IN ( SELECT lable FROM tmp_2_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_5 WHERE lable IN ( SELECT lable FROM tmp_1_5 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_1_0 WHERE lable IN ( SELECT lable FROM tmp_1_0 GROUP BY lable HAVING count(1)=groups ) UNION SELECT * FROM tmp_0_5 WHERE lable IN ( SELECT lable FROM tmp_0_5 GROUP BY lable HAVING count(1)=groups ) )tb )tb1 GROUP BY id,cinema_id,agent_id,cinema_name,area,addr,`area_name`, tele,longitude,latitude,cinema_brand,url,score,service,city_id,site_id, STATUS,`cinema_meta_id`,`unique_name` ORDER BY lable;END |delimiter ; 10. 过程调用方法两个过程的使用方法/第一个参数是城市编号,第二个参数是第一次打标签使用的范围值(此处0.003或0.002能筛选出的数据最多).第三个参数:4 跟爬去的站点数对应 /CALL batch_set_lable1(3120,0.002,4); /参数的含义是分几个站,跟爬去的站点数量对应.从表中拿出最终结果. /CALL cinema_result(4); 用贪心算法得出北上广三个城市的正确率:上海:86.67%北京:85.29%广州:78.09% 剩余的一些数据:需要人工比对…11. 根据贪心算法得出数据的准确率(此准确率是跟人工分组每组4个对比得出, 人工分组不等于4 小于4数据不完整,大于4此影院多拿一条数据我暂时认为数据非法, 哪怕机器分组 3 个,5个的跟人工的完全一致 , 也视为非法 )","tags":[{"name":"Work","slug":"Work","permalink":"http://yoursite.com/tags/Work/"}]},{"title":"MongoDB 分片","date":"2016-12-12T08:00:22.000Z","path":"2016/12/12/MongoDB-分片/","text":"1. 加host1234vi /etc/hosts192.168.130.93 mongo1192.168.130.94 mongo2192.168.130.95 mongo3 2. 创建目录123456mongo1mkdir -p /data/shard1_1 /data/shard2_1 /data/shard3_1 /data/configmongo2mkdir -p /data/shard1_2 /data/shard2_2 /data/shard3_2 /data/configmongo3mkdir -p /data/shard1_3 /data/shard2_3 /data/shard3_3 /data/config 3. mongo1配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_1/ --logpath /data/shard1_1/shard1_1.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_1/ --logpath /data/shard2_1/shard2_1.log --logappend --fork /usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_1/ --logpath /data/shard3_1/shard3_1.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 4. mongo2配置副本集,启动mongos路由.123456/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_2/ --logpath /data/shard1_2/shard1_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_2/ --logpath /data/shard2_2/shard2_2.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_2/ --logpath /data/shard3_2/shard3_2.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork 5. mongo3配置副本集,启动mongos路由.123456789101112131415161718/usr/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --journal --dbpath /data/shard1_3/ --logpath /data/shard1_3/shard1_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard2 --port 27018 --journal --dbpath /data/shard2_3/ --logpath /data/shard2_3/shard2_3.log --logappend --fork/usr/local/mongodb/bin/mongod --shardsvr --replSet shard3 --port 27019 --journal --dbpath /data/shard3_3/ --logpath /data/shard3_3/shard3_3.log --logappend --fork /usr/local/mongodb/bin/mongod --configsvr --dbpath /data/config --port 20000 --logpath /data/config/config.log --logappend --fork/usr/local/mongodb/bin/mongos -configdb mongo1:20000,mongo2:20000,mongo3:20000 --port 30000 --chunkSize 1 --logpath /data/mongos.log --logappend --fork --随便进入一台 27017 开启会议config=&#123;_id:&apos;shard1&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27017&apos;,priority:3&#125;, &#123;_id:1,host:&apos;mongo2:27017&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27017&apos;,priority:1&#125;]&#125;rs.initiate(config).--随便进入一台 27018 开启会议config=&#123;_id:&apos;shard2&apos;,members:[ &#123;_id:0,host:&apos;mongo1:27018&apos;,priority:1&#125;, &#123;_id:1,host:&apos;mongo2:27018&apos;,priority:2&#125;,&#123;_id:2,host:&apos;mongo3:27018&apos;,priority:3&#125;]&#125;rs.initiate(config)--随便进入一台 27019 开启会议config=&#123;_id:&apos;shard3’,members:[ &#123;_id:0,host:&apos;mongo1:27019’,priority:2&#125;, &#123;_id:1,host:&apos;mongo2:27019’,priority:3&#125;,&#123;_id:2,host:&apos;mongo3:27019’,priority:1&#125;]&#125;rs.initiate(config) 6. 添加分片12345mongo --port 30000 随便进入一台use admindb.runCommand(&#123;addshard:&quot;shard1/mongo1:27017,mongo2:27017,mongo3:27017&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard2/mongo1:27018,mongo2:27018,mongo3:27018&quot;&#125;)db.runCommand(&#123;addshard:&quot;shard3/mongo1:27019,mongo2:27019,mongo3:27019”&#125;) 7. test数据库开启Sharding1db.runCommand(&#123;enablesharding:&quot;test&quot;&#125;) 8. users 集合开启分片(要对一个集合分片,首先要对这个集合的数据库启用分片)1db.runCommand(&#123;shardcollection:&quot;test.users&quot;,key:&#123;_id:1&#125;&#125;)","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://yoursite.com/tags/MongoDB/"}]},{"title":"postGis 安装部署,对比geohash性能","date":"2016-12-09T05:47:14.000Z","path":"2016/12/09/postGis-安装部署-对比geohash性能/","text":"一、描述公司现获取最近楼栋信息用的是mongdb的geohash,想对比一下postgis和geohash的性能,首先假设我们安装好了PostgreSQL 9.5.2和Mongodb 3.0,mongodb的配置就不多少了,他本身集成geohash功能,安装配置也很简单,解压启动就OK了,postgis 相对比较麻烦,文献比较少,它依赖于很多插件(geos, proj,GDAL, libxml2, json-c,postgresql) 二、安装Postgis及插件1.安装geos12345tar -jxvf geos-3.5.0.tar.bz2cd geos-3.5.0./configuremakemake install 2.安装proj12345tar -zxvf proj.4-4.9.1.tar.gzcd proj.4-4.9.1./configuremakemake install 3.安装gdal12345tar -zxvf gdal-1.10.0.tar.gzcd gdal-1.10.0./configuremakemake install 4.安装libxml212345tar -zxvf libxml2-2.9.2.tar.gzcd libxml2-2.9.2./configuremakemake install 5. 安装json-c12345tar -zxvf json-c-json-c-0.11-20130402.tar.gzcd json-c-json-c-0.11-20130402./configuremakemake install 6. 安装postgis123456tar -zxvf postgis-2.1.8.tar.gzcd postgis-2.1.8# 这里要把前面装的插件全部配置上./configure --prefix=/usr/local/postgis --with-pgconfig=/usr/local/pgsql/bin/pg_config --with-xml2config=/usr/local/bin/xml2-config --with-geosconfig=/usr/local/bin/geos-config --with-gdalconfig=/usr/local/bin/gdal-configmakemake install 三、配置postgresql支持postgis1. postgresql 创建用户12create user luhuijun with password &apos;xxxxxx&apos;;grant all privileges on database customer to luhuijun; 2. 修改配置文件,因为postgresql的权限是文件/usr/local/pgsql/data/pg_hba.conf控制的,md5 密码认证,trust 免密码认证,这里跟mongodb又不一样,mongodb绑定的是自己的ip,内网,外网,回环(127.0.0.1),权限粒度大,内网段访问,外网段访问,本机访问, postgresql pg_hba.conf 配置文件host是访问者的ip,这个配置文件就能指定你只能访问那个数据库.加上role可以控制列的粒度,你只能访问一张表的某几个列. 3.把权限放大alter role luhuijun SUPERUSER; 4. 创建库,连接该数据库, 开启postgresql对postgis的支持1234create database custome;\\c customerCREATE EXTENSION postgis;CREATE EXTENSION postgis_topology; 现在就可以使用postgis 了. 四、查询脚本123 mongodb：db.runCommand(&#123;geoNear: &quot;gps_info&quot;, spherical: true, distanceMultiplier: 6378137, near: [120.6945, 27.998], num: 1, query: &#123;createdAt: &#123;$gt: &quot;2016-04-28&quot;&#125;&#125;&#125;);postgresql：select *,ST_Distance(jwd, ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163)) from building_gps_info order by jwd &lt;-&gt; ST_Transform(ST_GeomFromText(&apos;POINT(121.41011 31.17185)&apos;, 4326), 2163) limit 1; 五、Jmeter只读场景性能压测对比1. 在1核1GB的机器上,这是10个并发轮训100次对比结果,记录下来了平均响应时间,最小响应时间和最大响应时间.可以看到postgresql的平均响应时间比mongodb的平均响应时间快了28倍. 2. 在1核1GB的机器上,这是100个并发轮训100次得到的对比结果.可以看到伴随着并发量的增长,mongodb表现出来有些疲软,有些连接的最大响应时间达到了23.385秒,这应用应该已经慢到不能容忍了.并不像postgresql那么平稳. 六、 应用上的优势1. 酒后代驾,沿黄浦江画一条不规则的线,浦东的醉汉叫车时检索不到浦西的代驾司机,因为代驾司机的小电驴穿过江隧道比较吃力;2. 每30秒更新一次在线司机的经纬度信息，这是mongodb和mysql的myisam引擎做不到的，因为他们锁的最小粒度是表锁，更新的这段时间用户是无法下单的，能及时反馈你叫的司机离你有多远;3. 精准的分析，内环中环外环，某一片区域下了多少单，及时做推广;4. 业务员获取订单配送距离和推荐路线，需求点到点的距离计算、路径计算；5. 相似路径的多个订单的批量配送，需求位置和大量传统数据符合运算；6. 实时配送，位置跟踪。大量位置相关信息的存取，需要有较好的性能。七、把经纬度转换为Geo脚本–创建空间索引CREATE INDEX &quot;clean_cinema_data_idx&quot; ON &quot;public&quot;.&quot;clean_cinema_data&quot; USING gist(jwd); –动态修改经纬度脚本1select &apos;update clean_cinema_data set jwd=ST_GeomFromText(&apos;&apos;point(&apos;||longitude||&apos; &apos;||latitude||&apos;)&apos;&apos;,4326) where id=&apos;&apos;&apos;||ID||&apos;&apos;&apos;;&apos; from clean_cinema_data ; –查看效果select count(1) from clean_cinema_data where jwd is not null;","tags":[{"name":"postgis","slug":"postgis","permalink":"http://yoursite.com/tags/postgis/"}]},{"title":"Sqoop2 同步mysql 至 HDFS","date":"2016-12-08T02:14:53.000Z","path":"2016/12/08/Sqoop2-同步mysql-至-HDFS/","text":"1. 开启客户端sqoop2-shell 2. 配置sqoop server参数sqoop:000&gt; set server --host luhuijundeMacBook-Pro.local --port 12000 --webapp sqoop #luhuijundeMacBook-Pro.local 一般为HDFS主机名 –webapp官方文档说是指定的sqoop jetty服务器名称， 大概是一个自己能识别的用于标示这个服务器的名字吧。 3. 我们在使用的过程中可能会遇到错误，使用此配置打印错误sqoop:000&gt; set option --name verbose --value true 4. 验证是否已经连上sqoop:000&gt; show version --all #如果server version:能显示代表能正常连接 5. 使用help命令可以查看sqoop支持的所有命令1234567891011121314151617sqoop:000&gt; helpFor information about Sqoop, visit: http://sqoop.apache.org/Available commands: exit (\\x ) Exit the shell history (\\H ) Display, manage and recall edit-line history help (\\h ) Display this help message set (\\st ) Configure various client options and settings show (\\sh ) Display various objects and configuration options create (\\cr ) Create new object in Sqoop repository delete (\\d ) Delete existing object in Sqoop repository update (\\up ) Update objects in Sqoop repository clone (\\cl ) Create new object based on existing one start (\\sta) Start job stop (\\stp) Stop job status (\\stu) Display status of a job enable (\\en ) Enable object in Sqoop repository disable (\\di ) Disable object in Sqoop repository 6. 检查Sqoop server支持的连接sqoop:000&gt; show connector 7. 创建数据源头link12345678910111213141516sqoop:000&gt; create link -connector generic-jdbc-connectorName: First LinkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://mysql.server/databaseNameUsername:dbaPassword: *****Fetch Size:(回车)entry#protocol=tcpentry#(回车)Identifier enclose:(空格) #这里是指定SQL中标识符的定界符，也就是说，有的SQL标示符是一个引号：select * from &quot;table_name&quot;，这种定界符在MySQL中是会报错的。这个属性默认值就是双引号，所以不能使用回车，必须将之覆盖，我使用空格覆盖了这个值。官方文档这里有坑！New link was successfully created with validation status OK and name third link 看到这样的字符这个link算是创建成功 8. 创建目标link1234567891011sqoop:000&gt; create link -connector hdfs-connectorCreating link for connector with name hdfs-connectorPlease fill following values to create new link objectName: Second LinkLink configurationHDFS URI: hdfs://localhost:9000Conf directory:/usr/local/hadoop/etc/hadoopentry#(回车)New link was successfully created with validation status OK and name Second Link 看到这样的字符串代表创建成功 9. 使用两个link名字 from 和 to 来创建job12345678910111213141516171819202122232425262728293031323334353637383940414243444546sqoop:000&gt; create job -f &quot;First Link&quot; -t &quot;Second Link&quot; Creating job for links with from name First Link and to name Second Link Please fill following values to create new job object Name: Sqoopy #job 名称Schema name: test #mysql数据库名称Table name: test #表名称SQL statement:Column names:There are currently 0 values in the list:element#(回车)Partition column:(回车)Partition column nullable:(回车)Boundary query:(回车)Incremental readCheck column:(回车)Last value:(回车)Target configuration #配置目标Override null value: nullNull value: nullFile format: 0 : TEXT_FILE 1 : SEQUENCE_FILE 2 : PARQUET_FILEChoose: 0 #选择0最简单的文本文件Compression codec: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0 #选择0,不压缩Custom codec:(回车)Output directory: hdfs://localhost:9000/user/luhuijun/sqoop #最好是完全没有这个目录: sqoop,如果有目录里面又有文件, 又是一堆权限问题.Append mode:(回车)Throttling resourcesExtractors: 2Loaders: 2Classpath configurationExtra mapper jars:There are currently 0 values in the list:element#New job was successfully created with validation status OK and name Sqoopy 9. 开启job 并打印job执行详情12345678910111213141516sqoop:000&gt; start job -n Sqoopy -sSubmission detailsJob Name: SqoopyServer URL: http://luhuijundeMacBook-Pro.local:12000/sqoop/Created by: luhuijunCreation date: 2016-12-06 21:26:24 CSTLastly updated by: luhuijunExternal ID: job_1481030429951_0001 http://luhuijundeMacBook-Pro.local:8088/proxy/application_1481030429951_0001/Source Connector schema: Schema&#123;name= test . test ,columns=[ FixedPoint&#123;name=id,nullable=true,type=FIXED_POINT,byteSize=4,signed=true&#125;, Text&#123;name=name,nullable=true,type=TEXT,charSize=null&#125;]&#125;2016-12-06 21:26:24 CST: BOOTING - Progress is not available2016-12-06 21:26:36 CST: RUNNING - 0.00 %2016-12-06 21:26:46 CST: RUNNING - 50.00 %2016-12-06 21:26:57 CST: SUCCEEDED 10. 查看执行结果1234567➜ /usr/local/hadoop/etc/hadoop git:(master) &gt;hdfs dfs -cat &apos;sqoop/*&apos; #这是zsh *直接匹配不到,在正常的shell里应该不需要引号1,&apos;1&apos;2,&apos;xinle&apos;3,&apos;huijun&apos;4,&apos;hongna&apos; 11. 总结看上去简单的几步,其实踩了很多坑,在学习过程中通常会犯两类错误：第一 类错误是在知之不多的情况下就盲目开始,即行动太快；第二类错误是在行动之前准备过多,即行动太晚。要想在这 二者之间取得平衡，你掌握的知识要恰到好处，足以能让你开始学习， 但又不会多到让你无力探索，这样学习效果最佳。&lt;&lt;软技能 代码之外的生存指南&gt;&gt;","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]},{"title":"Sqoop2 version 1.99.7 安装部署","date":"2016-12-08T02:13:14.000Z","path":"2016/12/08/Sqoop2-version-1-99-7-安装部署/","text":"1. 版本对比Sqoop2 相比Sqoop1 升级幅度太大,可以说两个软件完全没有关系.Sqoop2 相比Sqoop1 增加了server端, sqoop1 是那种解压出来配置个环境变量就能直接使用的软件, sqoop2 安装部署使用复杂,而且官方给出来的文档有几个坑要踩踩. 2. Sqoop2 Version 1.99.7 安装部署下载软件:wget http://apache.fayea.com/sqoop/1.99.7/sqoop-1.99.7-bin-hadoop200.tar.gz 解压软件:tar -zvxf sqoop-1.99.7-bin-hadoop200.tar.gz 存放到指定路径:mv sqoop-1.99.7-bin-hadoop200 /usr/local/sqoop 赋权限:chmod -R 755 /usr/local/sqoop 修改环境变量:vim /etc/profile 最下方添加如下变量: 123456export SQOOP_HOME=/usr/local/sqoopexport PATH=$SQOOP_HOME/bin:$PATHexport CATALINA_BASE=$SQOOP_HOME/serverexport LOGDIR=$SQOOP_HOME/logsexport SQOOP_SERVER_EXTRA_LIB=/usr/local/sqoop/server/lib指定SQOOP_SERVER_EXTRA_LIB 后续要把连接mysql的jar包 copy 至这个文件加下 copy mysql-connector jar包至指定文件: cp /Users/luhuijun/Downloads/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar $SQOOP_HOME/server/lib 修改配置文件:sudo vim /usr/local/sqoop/conf/sqoop.properties 修改指向我的hadoop配置文件目录:org.apache.sqoop.submission.engine.mapreduce.configuration.directory=/usr/local/hadoop/etc/hadoop 配置catalina.properties 此文件不存在,需要自已建立:vim /usr/local/sqoop/conf/catalina.properties 内容如下: 1common.loader=/usr/local/hadoop/share/hadoop/common/*.jar,/usr/local/hadoop/share/hadoop/common/lib/*.jar,/usr/local/hadoop/share/hadoop/hdfs/*.jar,/usr/local/hadoop/share/hadoop/hdfs/lib/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/*.jar,/usr/local/hadoop/share/hadoop/mapreduce/lib/*.jar,/usr/local/hadoop/share/hadoop/tools/*.jar,/usr/local/hadoop/share/hadoop/tools/lib/*.jar,/usr/local/hadoop/share/hadoop/yarn/*.jar,/usr/local/hadoop/share/hadoop/yarn/lib/*.jar,/usr/local/hadoop/share/hadoop/httpfs/tomcat/lib/*.jar, 修改启动脚本: 12345678vim /usr/local/sqoop/bin/sqoop.sh添加如下内容,注意自己的路径:export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home HADOOP_COMMON_HOME=/usr/local/hadoop/share/hadoop/common HADOOP_HDFS_HOME=/usr/local/hadoop/share/hadoop/hdfs HADOOP_MAPRED_HOME=/usr/local/hadoop/share/hadoop/mapreduce HADOOP_YARN_HOME=/usr/local/hadoop/share/hadoop/yarn 修改hadoop的yarn-site.xml: 1234567vim /usr/local/hadoop/etc/hadoop/yarn-site.xml添加如下属性:&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 修改hadoop的container-executor.cfg: 1234vim /usr/local/hadoop/etc/hadoop/container-executor.cfg修改如些配置:allowed.system.users=luhuijun 配置为当前用户名, 不然sqoop2-shell去访问hdfs 文件的时候会报错(user: luhuijun is not allowed to impersonate luhuijun)这个问题坑了我好久 修改core-site.xml: 1234567891011vim /usr/local/hadoop/etc/hadoop/core-site.xml添加如下配置:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.groups&lt;/name&gt; &lt;value&gt;staff&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.luhuijun.hosts&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt;&lt;/property&gt; 跟修改上一个配置是解决同一个问题,sqoop2-shell去访问hdfs的时候会报(user: luhuijun is not allowed to impersonate luhuijun),staff 为当前系统用户的用户组, 使用groups能查到当前用户组名. 启动sqoop 服务:sqoop.sh server start 3. 验证启动是否成功12345sqoop.sh client 或 sqoop2-shell 进入客户端set server --host hadoopMaster --port 12000 --webapp sqoop 设置服务器，注意hadoopMaster为hdfs主机名show connector --all 查看连接类型show link 查看连接show job 查看job","tags":[{"name":"Sqoop","slug":"Sqoop","permalink":"http://yoursite.com/tags/Sqoop/"}]}]